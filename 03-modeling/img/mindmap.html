<!doctype html>
<html>
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta http-equiv="X-UA-Compatible" content="ie=edge" />
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
html {
  font-family: ui-sans-serif, system-ui, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.markmap-dark {
  background: #27272a;
  color: white;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.18.12/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/index.js"></script><script>((r) => {
          setTimeout(r);
        })(() => {
  const { markmap, mm } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute("style", "position:absolute;bottom:20px;right:20px");
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root2, jsonOptions) => {
              const markmap = getMarkmap();
              window.mm = markmap.Markmap.create(
                "svg#mindmap",
                (getOptions || markmap.deriveOptions)(jsonOptions),
                root2
              );
              if (window.matchMedia("(prefers-color-scheme: dark)").matches) {
                document.documentElement.classList.add("markmap-dark");
              }
            })(() => window.markmap,null,{"content":"Domain 3: Modeling (36% of Exam)","children":[{"content":"Task Statement 3.1: ML Problem Framing","children":[{"content":"1. The ML Pipeline Context","children":[{"content":"<strong>Previous Step (Domain 2)</strong>: Data Preprocessing &amp; Visualization","children":[],"payload":{"tag":"li","lines":"13,14"}},{"content":"<strong>Current Step (This Course)</strong>: Building, Training &amp; Testing the ML Model","children":[],"payload":{"tag":"li","lines":"14,15"}},{"content":"<strong>Exam Focus</strong>: Choosing the right algorithm for a business use case and knowing its performance metrics.","children":[],"payload":{"tag":"li","lines":"15,17"}}],"payload":{"tag":"h3","lines":"12,13"}},{"content":"2. When to Use and Not to Use ML","children":[{"content":"When ML Might Be a Good Fit","children":[{"content":"<strong>Requires Significant Data</strong>: ML needs large datasets to build a predictable model.","children":[],"payload":{"tag":"li","lines":"20,21"}},{"content":"<strong>Requires Expertise</strong>: Needs data processing and feature engineering to handle noise and retain meaningful information.","children":[],"payload":{"tag":"li","lines":"21,22"}},{"content":"<strong>Requires Powerful &amp; Scalable Machines</strong>: Needs computational power to crunch data and scale as data grows.","children":[],"payload":{"tag":"li","lines":"22,24"}}],"payload":{"tag":"h4","lines":"19,20"}},{"content":"When ML is NOT the Right Solution","children":[{"content":"<strong>Mission-Critical Applications</strong>: Scenarios where prediction errors are unacceptable.","children":[],"payload":{"tag":"li","lines":"25,26"}},{"content":"<strong>Simple Problems</strong>: Can be solved with simple rules and traditional programming (e.g., rule engines).","children":[],"payload":{"tag":"li","lines":"26,27"}},{"content":"<strong>Be Cognizant</strong>: ML is an expensive solution; ensure it&apos;s the right choice.","children":[],"payload":{"tag":"li","lines":"27,29"}}],"payload":{"tag":"h4","lines":"24,25"}}],"payload":{"tag":"h3","lines":"17,18"}},{"content":"3. Identifying the Right Learning Type","children":[{"content":"Supervised Learning &#x1f468;&#x200d;&#x1f3eb;","children":[{"content":"<strong>Analogy</strong>: A child learning under the guidance of a teacher.","children":[],"payload":{"tag":"li","lines":"32,33"}},{"content":"<strong>Data</strong>: Uses <strong>labeled training data</strong> (the outcome is already known).","children":[],"payload":{"tag":"li","lines":"33,34"}},{"content":"<strong>Goal</strong>: Model the relationship between inputs and outputs to predict new outcomes.","children":[],"payload":{"tag":"li","lines":"34,35"}},{"content":"<strong>Feedback</strong>: Possible, as the difference between actual and predicted values can be computed.","children":[],"payload":{"tag":"li","lines":"35,37"}}],"payload":{"tag":"h4","lines":"31,32"}},{"content":"Unsupervised Learning &#x1f575;&#xfe0f;","children":[{"content":"<strong>Analogy</strong>: A child figuring things out without supervision.","children":[],"payload":{"tag":"li","lines":"38,39"}},{"content":"<strong>Data</strong>: Deals with <strong>unlabeled data</strong>.","children":[],"payload":{"tag":"li","lines":"39,40"}},{"content":"<strong>Goal</strong>: Discover hidden structures, patterns, or information on its own.","children":[],"payload":{"tag":"li","lines":"40,41"}},{"content":"<strong>Feedback</strong>: Not possible.","children":[],"payload":{"tag":"li","lines":"41,43"}}],"payload":{"tag":"h4","lines":"37,38"}},{"content":"Reinforcement Learning &#x1f916;","children":[{"content":"<strong>Analogy</strong>: Rewarding a kid for good behavior to reinforce it.","children":[],"payload":{"tag":"li","lines":"44,45"}},{"content":"<strong>Core Idea</strong>: An autonomous, self-learning agent learns through trial and error in an interactive environment to achieve the most optimal results.","children":[],"payload":{"tag":"li","lines":"45,46"}},{"content":"<strong>Data</strong>: No labeled data; learns from feedback (rewards/penalties).","children":[],"payload":{"tag":"li","lines":"46,47"}},{"content":"<strong>Key Terminologies</strong>:","children":[{"content":"<strong>Agent</strong>: The entity learning to make decisions (e.g., a player in a maze).","children":[],"payload":{"tag":"li","lines":"48,49"}},{"content":"<strong>Environment</strong>: The problem space where the agent operates (e.g., the maze).","children":[],"payload":{"tag":"li","lines":"49,50"}},{"content":"<strong>State</strong>: The agent&apos;s current condition/position.","children":[],"payload":{"tag":"li","lines":"50,51"}},{"content":"<strong>Action</strong>: A choice made by the agent (e.g., move up, down, left, right).","children":[],"payload":{"tag":"li","lines":"51,52"}},{"content":"<strong>Reward/Penalty</strong>: Feedback from the environment based on an action.","children":[],"payload":{"tag":"li","lines":"52,53"}},{"content":"<strong>Policy</strong>: The agent&apos;s decision-making strategy to maximize rewards.","children":[],"payload":{"tag":"li","lines":"53,54"}}],"payload":{"tag":"li","lines":"47,54"}},{"content":"<strong>Types</strong>:","children":[{"content":"<strong>Model-Based</strong>: Agent builds an internal model (a &quot;mental map&quot;) of the environment to plan actions.","children":[],"payload":{"tag":"li","lines":"55,56"}},{"content":"<strong>Model-Free</strong>: Agent learns directly through trial and error without building an explicit model.","children":[],"payload":{"tag":"li","lines":"56,57"}}],"payload":{"tag":"li","lines":"54,57"}},{"content":"<strong>Use Cases</strong>: Self-driving vehicles, robotics, dynamic pricing, supply chain optimization.","children":[],"payload":{"tag":"li","lines":"57,59"}}],"payload":{"tag":"h4","lines":"43,44"}}],"payload":{"tag":"h3","lines":"29,30"}},{"content":"4. Selecting the Right Model Type","children":[{"content":"Classification","children":[{"content":"<strong>Use Case</strong>: When the dependent feature is categorical (discrete classes).","children":[],"payload":{"tag":"li","lines":"62,63"}},{"content":"<strong>Analogy</strong>: A fruit vendor sorting fruits as &apos;good&apos; or &apos;bad&apos;.","children":[],"payload":{"tag":"li","lines":"63,64"}},{"content":"<strong>Types</strong>:","children":[{"content":"<strong>Binary Classification</strong>: Two possible outcomes (e.g., Yes/No, Fraud/Legitimate).","children":[{"content":"<em>Algorithms</em>: Logistic Regression, Support Vector Machines (SVM).","children":[],"payload":{"tag":"li","lines":"66,67"}}],"payload":{"tag":"li","lines":"65,67"}},{"content":"<strong>Multiclass Classification</strong>: More than two mutually exclusive classes (e.g., sorting fruits into &apos;apples&apos;, &apos;oranges&apos;, &apos;bananas&apos;).","children":[{"content":"<em>Algorithms</em>: K-Nearest Neighbors (KNN), Naive Bayes.","children":[],"payload":{"tag":"li","lines":"68,69"}}],"payload":{"tag":"li","lines":"67,69"}},{"content":"<strong>Multilabel Classification</strong>: A data point can be assigned multiple labels (e.g., tagging a movie with genres like &apos;action&apos;, &apos;comedy&apos;, &apos;sci-fi&apos;).","children":[{"content":"<em>Algorithms</em>: Ensemble methods, Deep Learning.","children":[],"payload":{"tag":"li","lines":"70,71"}}],"payload":{"tag":"li","lines":"69,71"}}],"payload":{"tag":"li","lines":"64,71"}},{"content":"<strong>Challenge: Imbalanced Data</strong>","children":[{"content":"<strong>Problem</strong>: One class (minority) is significantly smaller than another (majority), causing biased models.","children":[],"payload":{"tag":"li","lines":"72,73"}},{"content":"<strong>Solution</strong>: <strong>SMOTE</strong> (Synthetic Minority Over-sampling TEchnique) to generate new synthetic samples for the minority class.","children":[],"payload":{"tag":"li","lines":"73,74"}}],"payload":{"tag":"li","lines":"71,74"}},{"content":"<strong>Learner Types</strong>:","children":[{"content":"<strong>Eager Learners</strong>: Build a model during training; fast predictions (e.g., Logistic Regression, SVM).","children":[],"payload":{"tag":"li","lines":"75,76"}},{"content":"<strong>Lazy Learners</strong>: Memorize training data; slow predictions (e.g., KNN).","children":[],"payload":{"tag":"li","lines":"76,77"}}],"payload":{"tag":"li","lines":"74,77"}},{"content":"<strong>Common Algorithms</strong>:","children":[{"content":"<strong>Logistic Regression</strong>: Predicts a binary outcome using a sigmoid (S-shaped) curve. Computationally efficient but sensitive to outliers.","children":[],"payload":{"tag":"li","lines":"78,79"}},{"content":"<strong>Naive Bayes</strong>: Based on Bayes&apos; theorem with an assumption of feature independence. Fast and handles missing data, but the independence assumption is a drawback.","children":[],"payload":{"tag":"li","lines":"79,80"}},{"content":"<strong>Support Vector Machines (SVM)</strong>: Finds an optimal hyperplane to separate classes. Generalizes well but is computationally expensive and memory-intensive.","children":[],"payload":{"tag":"li","lines":"80,81"}},{"content":"<strong>K-Nearest Neighbors (KNN)</strong>: Classifies new data based on the labels of its &apos;K&apos; nearest neighbors. No training phase but is memory-intensive.","children":[],"payload":{"tag":"li","lines":"81,83"}}],"payload":{"tag":"li","lines":"77,83"}}],"payload":{"tag":"h4","lines":"61,62"}},{"content":"Regression","children":[{"content":"<strong>Use Case</strong>: When the target feature is quantitative or continuous.","children":[],"payload":{"tag":"li","lines":"84,85"}},{"content":"<strong>Example</strong>: Predicting house prices based on features like square footage.","children":[],"payload":{"tag":"li","lines":"85,86"}},{"content":"<strong>Variations</strong>:","children":[{"content":"<strong>Linear Regression</strong>: One dependent and one independent feature (<code>Y = mx + b</code>).","children":[],"payload":{"tag":"li","lines":"87,88"}},{"content":"<strong>Multiple Regression</strong>: One dependent and multiple independent features (<code>Y = m1x1 + m2x2 + ... + b</code>).","children":[],"payload":{"tag":"li","lines":"88,89"}},{"content":"<strong>Polynomial Regression</strong>: Used when the relationship is non-linear (e.g., <code>Y = m1x1^2 + m2x2 + b</code>).","children":[],"payload":{"tag":"li","lines":"89,91"}}],"payload":{"tag":"li","lines":"86,91"}}],"payload":{"tag":"h4","lines":"83,84"}},{"content":"Time Series Forecasting &#x1f4c8;","children":[{"content":"<strong>Use Case</strong>: For data collected at regular time intervals (e.g., stock prices, sales data).","children":[],"payload":{"tag":"li","lines":"92,93"}},{"content":"<strong>Key Feature</strong>: <strong>Time</strong> is always an independent feature.","children":[],"payload":{"tag":"li","lines":"93,94"}},{"content":"<strong>Core Components</strong>:","children":[{"content":"<strong>Trend</strong>: The direction of data over time (upward, downward).","children":[],"payload":{"tag":"li","lines":"95,96"}},{"content":"<strong>Seasonality</strong>: Periodic fluctuations at regular intervals (e.g., higher sales in winter).","children":[],"payload":{"tag":"li","lines":"96,97"}},{"content":"<strong>Cyclical Variations</strong>: Non-repeating fluctuations at irregular intervals (e.g., economic cycles).","children":[],"payload":{"tag":"li","lines":"97,98"}},{"content":"<strong>Irregularity</strong>: Randomness or noise in the data.","children":[],"payload":{"tag":"li","lines":"98,99"}}],"payload":{"tag":"li","lines":"94,99"}},{"content":"<strong>Data Types</strong>:","children":[{"content":"<strong>Stationary Data</strong>: Statistical properties (mean, variance) are constant over time. Easier to model.","children":[],"payload":{"tag":"li","lines":"100,101"}},{"content":"<strong>Non-Stationary Data</strong>: Statistical properties change over time. Requires transformation (e.g., differencing) to make it stationary.","children":[],"payload":{"tag":"li","lines":"101,102"}}],"payload":{"tag":"li","lines":"99,102"}},{"content":"<strong>Limitations</strong>: Sensitive to missing data, assumes linear relationships sometimes, relies heavily on historical data.","children":[],"payload":{"tag":"li","lines":"102,104"}}],"payload":{"tag":"h4","lines":"91,92"}},{"content":"Clustering","children":[{"content":"<strong>Type</strong>: Unsupervised learning algorithm.","children":[],"payload":{"tag":"li","lines":"105,106"}},{"content":"<strong>Goal</strong>: Group similar data points into clusters based on similarity (distance).","children":[],"payload":{"tag":"li","lines":"106,107"}},{"content":"<strong>Hard vs. Soft Clustering</strong>:","children":[{"content":"<strong>Hard Clustering</strong>: Each data point belongs to only one cluster (e.g., K-Means).","children":[],"payload":{"tag":"li","lines":"108,109"}},{"content":"<strong>Soft Clustering</strong>: Each data point has a probability of belonging to each cluster (e.g., Fuzzy C-Means).","children":[],"payload":{"tag":"li","lines":"109,110"}}],"payload":{"tag":"li","lines":"107,110"}},{"content":"<strong>Categories</strong>:","children":[{"content":"<strong>Centroid-based (e.g., K-Means)</strong>: Requires a predetermined number of clusters (<code>k</code>).","children":[],"payload":{"tag":"li","lines":"111,112"}},{"content":"<strong>Density-based (e.g., DBScan)</strong>: Does not require a predetermined number of clusters; good at handling outliers.","children":[],"payload":{"tag":"li","lines":"112,113"}},{"content":"<strong>Hierarchical</strong>: Builds a hierarchy of clusters (a <code>dendrogram</code>). Can be agglomerative (bottom-up) or divisive (top-down).","children":[],"payload":{"tag":"li","lines":"113,114"}},{"content":"<strong>Distribution-based (e.g., Gaussian Mixture Model)</strong>: Assumes data comes from a mixture of probability distributions.","children":[],"payload":{"tag":"li","lines":"114,115"}}],"payload":{"tag":"li","lines":"110,115"}},{"content":"<strong>Use Cases</strong>: Customer segmentation, anomaly/fraud detection, document organization.","children":[],"payload":{"tag":"li","lines":"115,117"}}],"payload":{"tag":"h4","lines":"104,105"}},{"content":"Association Learning","children":[{"content":"<strong>Type</strong>: Unsupervised learning.","children":[],"payload":{"tag":"li","lines":"118,119"}},{"content":"<strong>Goal</strong>: Discover hidden patterns and relationships between features (e.g., &quot;if a customer buys X, they are likely to buy Y&quot;).","children":[],"payload":{"tag":"li","lines":"119,120"}},{"content":"<strong>Analogy</strong>: A vendor notices people who buy apples and oranges also buy bananas, so they place them together.","children":[],"payload":{"tag":"li","lines":"120,121"}},{"content":"<strong>Terminology</strong>:","children":[{"content":"<strong>Rule</strong>: <code>If {Antecedent} -&gt; Then {Consequent}</code>","children":[],"payload":{"tag":"li","lines":"122,123"}},{"content":"<strong>Metrics</strong>:","children":[{"content":"<strong>Support</strong>: Frequency of items occurring together.","children":[],"payload":{"tag":"li","lines":"124,125"}},{"content":"<strong>Confidence</strong>: Likelihood that the consequent is purchased when the antecedent is.","children":[],"payload":{"tag":"li","lines":"125,126"}},{"content":"<strong>Lift</strong>: Strength of the association (&gt;1 means positive correlation).","children":[],"payload":{"tag":"li","lines":"126,127"}}],"payload":{"tag":"li","lines":"123,127"}}],"payload":{"tag":"li","lines":"121,127"}},{"content":"<strong>Algorithms</strong>: Apriori, FP-Growth, Eclat.","children":[],"payload":{"tag":"li","lines":"127,128"}},{"content":"<strong>Use Cases</strong>: Market basket analysis, recommendation systems, supply chain management.","children":[],"payload":{"tag":"li","lines":"128,130"}}],"payload":{"tag":"h4","lines":"117,118"}},{"content":"Advanced: Deep Learning &amp; Neural Networks &#x1f9e0;","children":[{"content":"<strong>Purpose</strong>: A subset of ML that addresses limitations of traditional ML, like handling large/complex data and non-linear relationships.","children":[],"payload":{"tag":"li","lines":"131,132"}},{"content":"<strong>Concept</strong>: Modeled after the human brain, using artificial neural networks with multiple layers.","children":[],"payload":{"tag":"li","lines":"132,133"}},{"content":"<strong>Artificial Neuron (Perceptron)</strong>:","children":[{"content":"<strong>Components</strong>: Inputs, Weights, Bias, Net Sum, Activation Function.","children":[],"payload":{"tag":"li","lines":"134,135"}},{"content":"<strong>Analogy to Biological Neuron</strong>: Dendrites (Inputs), Axon (Output), Synapse (Weights).","children":[],"payload":{"tag":"li","lines":"135,136"}}],"payload":{"tag":"li","lines":"133,136"}},{"content":"<strong>Activation Functions</strong>: Introduce non-linearity.","children":[{"content":"<em>Examples</em>: Sigmoid (S-shaped curve for probability), Tanh, <strong>ReLU</strong> (Rectified Linear Unit - fast and effective), Softmax.","children":[],"payload":{"tag":"li","lines":"137,138"}}],"payload":{"tag":"li","lines":"136,138"}},{"content":"<strong>Neural Network Architecture</strong>:","children":[{"content":"<strong>Layers</strong>: Input Layer, Hidden Layer(s), Output Layer.","children":[],"payload":{"tag":"li","lines":"139,140"}},{"content":"<strong>Training Process</strong>:","children":[{"content":"<strong>Forward Propagation</strong>: Data flows from input to output.","children":[],"payload":{"tag":"li","lines":"141,142"}},{"content":"<strong>Cost Function</strong>: Measures the error in prediction.","children":[],"payload":{"tag":"li","lines":"142,143"}},{"content":"<strong>Backward Propagation</strong>: Error is fed back to adjust weights and biases.","children":[],"payload":{"tag":"li","lines":"143,144"}}],"payload":{"tag":"li","lines":"140,144"}}],"payload":{"tag":"li","lines":"138,144"}},{"content":"<strong>Challenges</strong>: Requires high computational power, large amounts of data, and careful hyperparameter tuning.","children":[],"payload":{"tag":"li","lines":"144,145"}},{"content":"<strong>Techniques</strong>:","children":[{"content":"<strong>CNN (Convolutional Neural Networks)</strong>:","children":[{"content":"<strong>Use Case</strong>: Ideal for image and video processing.","children":[],"payload":{"tag":"li","lines":"147,148"}},{"content":"<strong>Architecture</strong>:","children":[{"content":"1. <strong>Convolution Layer</strong>: Applies filters (kernels) to extract features (creates a feature map).","children":[],"payload":{"tag":"li","lines":"149,150","listIndex":1}},{"content":"2. <strong>Pooling Layer</strong>: Downsamples the feature map (e.g., MaxPooling).","children":[],"payload":{"tag":"li","lines":"150,151","listIndex":2}},{"content":"3. <strong>Flattening Layer</strong>: Converts 2D data to a 1D vector.","children":[],"payload":{"tag":"li","lines":"151,152","listIndex":3}},{"content":"4. <strong>Fully Connected Layer</strong>: Performs classification based on extracted features.","children":[],"payload":{"tag":"li","lines":"152,153","listIndex":4}}],"payload":{"tag":"li","lines":"148,153"}}],"payload":{"tag":"li","lines":"146,153"}},{"content":"<strong>RNN (Recurrent Neural Networks)</strong>:","children":[{"content":"<strong>Use Case</strong>: Designed to process sequential data (text, time series, audio) by using an internal memory.","children":[],"payload":{"tag":"li","lines":"154,155"}},{"content":"<strong>Key Idea</strong>: Information flows in loops, allowing the network to persist information from previous steps.","children":[],"payload":{"tag":"li","lines":"155,156"}},{"content":"<strong>Architectures</strong>: One-to-One, One-to-Many (Image Captioning), Many-to-One (Sentiment Analysis), Many-to-Many (Translation).","children":[],"payload":{"tag":"li","lines":"156,157"}},{"content":"<strong>Advanced RNNs</strong>:","children":[{"content":"<strong>LSTM (Long Short-Term Memory)</strong>: Solves RNN&apos;s problem with long-range dependencies using a system of &apos;gates&apos;.","children":[],"payload":{"tag":"li","lines":"158,159"}},{"content":"<strong>GRU (Gated Recurrent Unit)</strong>: Similar to LSTM but with a simpler architecture, making it computationally less expensive.","children":[],"payload":{"tag":"li","lines":"159,160"}}],"payload":{"tag":"li","lines":"157,160"}}],"payload":{"tag":"li","lines":"153,160"}},{"content":"<strong>Transfer Learning</strong>:","children":[{"content":"<strong>Concept</strong>: Reusing a pre-trained model on a new, related task.","children":[],"payload":{"tag":"li","lines":"161,162"}},{"content":"<strong>Analogy</strong>: A Java expert learning Python reuses fundamental programming concepts.","children":[],"payload":{"tag":"li","lines":"162,163"}},{"content":"<strong>Process</strong>: Freeze the weights of early layers (feature extraction) and retrain the final task-specific layers on new data.","children":[],"payload":{"tag":"li","lines":"163,164"}},{"content":"<strong>Benefits</strong>: Reduces data scarcity issues, saves computational resources, and enhances performance.","children":[],"payload":{"tag":"li","lines":"164,166"}}],"payload":{"tag":"li","lines":"160,166"}}],"payload":{"tag":"li","lines":"145,166"}}],"payload":{"tag":"h4","lines":"130,131"}}],"payload":{"tag":"h3","lines":"59,60"}}],"payload":{"tag":"h2","lines":"10,11"}},{"content":"Task Statement 3.2: Amazon SageMaker Built-in Algorithms","children":[{"content":"Guiding Principle","children":[{"content":"The data type in the question will help narrow down your algorithm choices.","children":[],"payload":{"tag":"li","lines":"171,173"}}],"payload":{"tag":"h3","lines":"170,171"}},{"content":"1. Algorithms for Tabular Data","children":[{"content":"<strong>Definition</strong>: Datasets organized in tables (rows/observations, columns/features).","children":[],"payload":{"tag":"li","lines":"174,175"}},{"content":"<strong>Algorithms</strong>:","children":[{"content":"XGBoost","children":[],"payload":{"tag":"li","lines":"176,177"}},{"content":"Linear Learner","children":[],"payload":{"tag":"li","lines":"177,178"}},{"content":"K-Nearest Neighbor (KNN)","children":[],"payload":{"tag":"li","lines":"178,179"}},{"content":"Factorization Machines","children":[],"payload":{"tag":"li","lines":"179,181"}}],"payload":{"tag":"li","lines":"175,181"}}],"payload":{"tag":"h3","lines":"173,174"}},{"content":"2. Algorithms for Time Series Data","children":[{"content":"<strong>Definition</strong>: Data recorded over consistent time intervals.","children":[],"payload":{"tag":"li","lines":"182,183"}},{"content":"<strong>Examples</strong>: Forecasting product demand, analyzing server loads.","children":[],"payload":{"tag":"li","lines":"183,184"}},{"content":"<strong>Algorithm</strong>:","children":[{"content":"DeepAR","children":[],"payload":{"tag":"li","lines":"185,187"}}],"payload":{"tag":"li","lines":"184,187"}}],"payload":{"tag":"h3","lines":"181,182"}},{"content":"3. Algorithms for Unsupervised Learning","children":[{"content":"<strong>Use Cases</strong>: Unlabeled data for clustering, dimensionality reduction, anomaly detection.","children":[],"payload":{"tag":"li","lines":"188,189"}},{"content":"<strong>Algorithms</strong>:","children":[{"content":"Principal Component Analysis (PCA)","children":[],"payload":{"tag":"li","lines":"190,191"}},{"content":"Random Cut Forest","children":[],"payload":{"tag":"li","lines":"191,192"}},{"content":"IP Insights","children":[],"payload":{"tag":"li","lines":"192,193"}},{"content":"K-Means","children":[],"payload":{"tag":"li","lines":"193,195"}}],"payload":{"tag":"li","lines":"189,195"}}],"payload":{"tag":"h3","lines":"187,188"}},{"content":"4. Algorithms for Text Data (NLP)","children":[{"content":"<strong>Use Cases</strong>: Document summarization, topic modeling, language translation.","children":[],"payload":{"tag":"li","lines":"196,197"}},{"content":"<strong>Algorithms</strong>:","children":[{"content":"Object2Vec","children":[],"payload":{"tag":"li","lines":"198,199"}},{"content":"Latent Dirichlet Allocation (LDA)","children":[],"payload":{"tag":"li","lines":"199,200"}},{"content":"Neural Topic Model (NTM)","children":[],"payload":{"tag":"li","lines":"200,201"}},{"content":"BlazingText","children":[],"payload":{"tag":"li","lines":"201,202"}},{"content":"Sequence-to-Sequence","children":[],"payload":{"tag":"li","lines":"202,204"}}],"payload":{"tag":"li","lines":"197,204"}}],"payload":{"tag":"h3","lines":"195,196"}},{"content":"5. Algorithms for Image Data","children":[{"content":"<strong>Use Cases</strong>: Analyze and process image data.","children":[],"payload":{"tag":"li","lines":"205,206"}},{"content":"<strong>Algorithms</strong>:","children":[{"content":"Image Classification","children":[],"payload":{"tag":"li","lines":"207,208"}},{"content":"Object Detection","children":[],"payload":{"tag":"li","lines":"208,209"}},{"content":"Semantic Segmentation","children":[],"payload":{"tag":"li","lines":"209,211"}}],"payload":{"tag":"li","lines":"206,211"}}],"payload":{"tag":"h3","lines":"204,205"}}],"payload":{"tag":"h2","lines":"168,169"}},{"content":"Task Statement 3.3: Model Training &amp; Optimization","children":[{"content":"1. Splitting Data","children":[{"content":"<strong>Training Data</strong>: Used to train the model.","children":[],"payload":{"tag":"li","lines":"216,217"}},{"content":"<strong>Validation Data (Optional)</strong>: Measures model performance during training and tunes hyperparameters.","children":[],"payload":{"tag":"li","lines":"217,218"}},{"content":"<strong>Testing Data</strong>: Determines how well the model generalizes to unseen data.","children":[],"payload":{"tag":"li","lines":"218,219"}},{"content":"<strong>Cross-Validation</strong>: Process of validating the model against fresh, unseen data.","children":[{"content":"<strong>Techniques</strong>:","children":[{"content":"K-fold Cross-Validation","children":[],"payload":{"tag":"li","lines":"221,222"}},{"content":"Stratified K-fold Cross-Validation","children":[],"payload":{"tag":"li","lines":"222,223"}},{"content":"Leave-one-out Cross-Validation","children":[],"payload":{"tag":"li","lines":"223,225"}}],"payload":{"tag":"li","lines":"220,225"}}],"payload":{"tag":"li","lines":"219,225"}}],"payload":{"tag":"h3","lines":"215,216"}},{"content":"2. Optimization Techniques","children":[{"content":"<strong>Loss Function</strong>: Measures model accuracy (difference between predicted and actual output).","children":[],"payload":{"tag":"li","lines":"226,227"}},{"content":"<strong>Optimization Goal</strong>: Minimize the loss function for quick model convergence.","children":[],"payload":{"tag":"li","lines":"227,228"}},{"content":"<strong>Gradient Descent</strong>: A common optimization technique.","children":[{"content":"<strong>Challenge</strong>: May get stuck in local minima.","children":[],"payload":{"tag":"li","lines":"229,230"}},{"content":"<strong>Solutions</strong>:","children":[{"content":"Stochastic Gradient Descent","children":[],"payload":{"tag":"li","lines":"231,232"}},{"content":"Batch Gradient Descent","children":[],"payload":{"tag":"li","lines":"232,234"}}],"payload":{"tag":"li","lines":"230,234"}}],"payload":{"tag":"li","lines":"228,234"}}],"payload":{"tag":"h3","lines":"225,226"}},{"content":"3. Choosing Compute Resources","children":[{"content":"<strong>CPUs are good for</strong>:","children":[{"content":"Simpler classification/regression problems.","children":[],"payload":{"tag":"li","lines":"236,237"}},{"content":"Smaller models with fewer parameters.","children":[],"payload":{"tag":"li","lines":"237,238"}},{"content":"Low latency requirements.","children":[],"payload":{"tag":"li","lines":"238,239"}},{"content":"Severe budget constraints.","children":[],"payload":{"tag":"li","lines":"239,240"}}],"payload":{"tag":"li","lines":"235,240"}},{"content":"<strong>GPUs are good for</strong>:","children":[{"content":"Complex models (Deep Learning).","children":[],"payload":{"tag":"li","lines":"241,242"}},{"content":"Models with a large number of parameters.","children":[],"payload":{"tag":"li","lines":"242,243"}},{"content":"High throughput requirements.","children":[],"payload":{"tag":"li","lines":"243,244"}},{"content":"Significant performance requirements.","children":[],"payload":{"tag":"li","lines":"244,245"}}],"payload":{"tag":"li","lines":"240,245"}},{"content":"<strong>Distributed Training</strong>: For complex models on large datasets across multiple instances.","children":[{"content":"<strong>SageMaker Strategies</strong>: Data Parallelism &amp; Model Parallelism.","children":[],"payload":{"tag":"li","lines":"246,247"}},{"content":"<strong>SageMaker Tools</strong>: Prebuilt Docker images with Apache Spark.","children":[],"payload":{"tag":"li","lines":"247,249"}}],"payload":{"tag":"li","lines":"245,249"}}],"payload":{"tag":"h3","lines":"234,235"}},{"content":"4. Updating &amp; Retraining Models","children":[{"content":"<strong>Importance</strong>: Retraining with the latest data keeps models up-to-date and accurate.","children":[],"payload":{"tag":"li","lines":"250,251"}},{"content":"<strong>Amazon SageMaker Canvas</strong>:","children":[{"content":"Drag-and-drop UI for non-technical users.","children":[],"payload":{"tag":"li","lines":"252,253"}},{"content":"Offers features for manual or automatically scheduled model updates.","children":[],"payload":{"tag":"li","lines":"253,255"}}],"payload":{"tag":"li","lines":"251,255"}}],"payload":{"tag":"h3","lines":"249,250"}}],"payload":{"tag":"h2","lines":"213,214"}},{"content":"Task Statement 3.4: Hyperparameter Tuning &amp; Model Concepts","children":[{"content":"1. Regularization","children":[{"content":"<strong>Goal</strong>: Prevent overfitting and improve model performance.","children":[],"payload":{"tag":"li","lines":"260,261"}},{"content":"<strong>Overfitting</strong>: Performs well on training data, poorly on new data.","children":[],"payload":{"tag":"li","lines":"261,262"}},{"content":"<strong>Underfitting</strong>: Unable to learn hidden patterns in the data.","children":[],"payload":{"tag":"li","lines":"262,263"}},{"content":"<strong>Techniques</strong>:","children":[{"content":"<strong>L1 Regularization</strong>: Adds sum of <em>absolute</em> values of coefficients to the loss function. Good for minimizing impact of irrelevant features.","children":[],"payload":{"tag":"li","lines":"264,265"}},{"content":"<strong>L2 Regularization</strong>: Adds sum of <em>squared</em> values of coefficients to the loss function. Distributes the impact of all important features.","children":[],"payload":{"tag":"li","lines":"265,266"}},{"content":"<strong>Early Stopping</strong>: Stops training when performance on a validation set stops improving.","children":[],"payload":{"tag":"li","lines":"266,268"}}],"payload":{"tag":"li","lines":"263,268"}}],"payload":{"tag":"h3","lines":"259,260"}},{"content":"2. Cross-Validation","children":[{"content":"<strong>Goal</strong>: Prevent overfitting by assessing performance and generalizability, especially with limited data.","children":[],"payload":{"tag":"li","lines":"269,270"}},{"content":"<strong>Concept</strong>: Partitions the dataset into training and testing subsets to train on different parts of the data.","children":[],"payload":{"tag":"li","lines":"270,271"}},{"content":"<strong>Techniques</strong>:","children":[{"content":"<strong>K-fold</strong>: Dataset split into K folds. Train on K-1, test on the Kth fold. Repeat K times.","children":[],"payload":{"tag":"li","lines":"272,273"}},{"content":"<strong>Stratified K-fold</strong>: Like K-fold, but each fold maintains the same class distribution as the original dataset. Effective for imbalanced data.","children":[],"payload":{"tag":"li","lines":"273,274"}},{"content":"<strong>Time Series</strong>: Folds are created sequentially based on time, ensuring chronological order. Effective when data order is important.","children":[],"payload":{"tag":"li","lines":"274,276"}}],"payload":{"tag":"li","lines":"271,276"}}],"payload":{"tag":"h3","lines":"268,269"}},{"content":"3. Initializing Models for Tuning","children":[{"content":"<strong>Process</strong>: Initialize algorithms with a starting set of hyperparameter values.","children":[],"payload":{"tag":"li","lines":"277,278"}},{"content":"<strong>Ranges</strong>: Tuning jobs search for the best values over defined ranges.","children":[],"payload":{"tag":"li","lines":"278,279"}},{"content":"<strong>Range Types</strong>:","children":[{"content":"Categorical Parameter","children":[],"payload":{"tag":"li","lines":"280,281"}},{"content":"Continuous Parameter","children":[],"payload":{"tag":"li","lines":"281,282"}},{"content":"Integer Parameter","children":[],"payload":{"tag":"li","lines":"282,284"}}],"payload":{"tag":"li","lines":"279,284"}}],"payload":{"tag":"h3","lines":"276,277"}},{"content":"4. Neural Network Architecture","children":[{"content":"<strong>Components</strong>:","children":[{"content":"<strong>Input Layer</strong>: Receives data.","children":[],"payload":{"tag":"li","lines":"286,287"}},{"content":"<strong>Hidden Layer(s)</strong>: Transform input data.","children":[],"payload":{"tag":"li","lines":"287,288"}},{"content":"<strong>Output Layer</strong>: Makes final predictions.","children":[],"payload":{"tag":"li","lines":"288,289"}}],"payload":{"tag":"li","lines":"285,289"}},{"content":"<strong>Key Concepts</strong>:","children":[{"content":"<strong>Weights</strong>: Determine feature importance; adjusted during training.","children":[],"payload":{"tag":"li","lines":"290,291"}},{"content":"<strong>Biases</strong>: One per neuron; helps the model fit the data better.","children":[],"payload":{"tag":"li","lines":"291,292"}},{"content":"<strong>Activation Function</strong>: Introduces non-linearity to learn complex patterns.","children":[{"content":"<em>Examples</em>: Sigmoid, Tanh, ReLU, Softmax.","children":[],"payload":{"tag":"li","lines":"293,295"}}],"payload":{"tag":"li","lines":"292,295"}}],"payload":{"tag":"li","lines":"289,295"}}],"payload":{"tag":"h3","lines":"284,285"}},{"content":"5. Understanding Tree-Based Models","children":[{"content":"<strong>Type</strong>: Supervised learning algorithm that builds a tree structure.","children":[],"payload":{"tag":"li","lines":"296,297"}},{"content":"<strong>Structure</strong>:","children":[{"content":"<strong>Root Node</strong>: Top node, represents the entire dataset.","children":[],"payload":{"tag":"li","lines":"298,299"}},{"content":"<strong>Sub-nodes</strong>: Intermediate decision points that split the data.","children":[],"payload":{"tag":"li","lines":"299,300"}},{"content":"<strong>Branches/Edges</strong>: Represent the outcomes of a decision.","children":[],"payload":{"tag":"li","lines":"300,301"}},{"content":"<strong>Leaf Nodes</strong>: Terminal nodes representing the final outcome.","children":[],"payload":{"tag":"li","lines":"301,302"}}],"payload":{"tag":"li","lines":"297,302"}},{"content":"<strong>Key Hyperparameters</strong>:","children":[{"content":"<code>no_of_folds</code>","children":[],"payload":{"tag":"li","lines":"303,304"}},{"content":"<code>max_depth</code>","children":[],"payload":{"tag":"li","lines":"304,305"}},{"content":"<code>min_samples_split</code>","children":[],"payload":{"tag":"li","lines":"305,306"}},{"content":"<code>min_samples_leaf</code>","children":[],"payload":{"tag":"li","lines":"306,308"}}],"payload":{"tag":"li","lines":"302,308"}}],"payload":{"tag":"h3","lines":"295,296"}},{"content":"6. Understanding Linear Models","children":[{"content":"<strong>Gradient Descent</strong>: Advanced algorithm to find optimal hyperparameter values.","children":[],"payload":{"tag":"li","lines":"309,310"}},{"content":"<strong>Key Hyperparameters</strong>:","children":[{"content":"<strong>Learning Rate</strong>: Critical for the efficiency of the optimization process.","children":[],"payload":{"tag":"li","lines":"311,312"}},{"content":"<strong>Alpha</strong>: Controls regularization strength in Ridge Regression to balance bias and variance.","children":[],"payload":{"tag":"li","lines":"312,314"}}],"payload":{"tag":"li","lines":"310,314"}}],"payload":{"tag":"h3","lines":"308,309"}}],"payload":{"tag":"h2","lines":"257,258"}},{"content":"Task Statement 3.5: Model Evaluation","children":[{"content":"Note on Overlapping Topics","children":[{"content":"Overfitting, underfitting, and cross-validation are covered in detail under <strong>Task Statement 3.4</strong>.","children":[],"payload":{"tag":"li","lines":"319,321"}}],"payload":{"tag":"h3","lines":"318,319"}},{"content":"1. Evaluate Metrics","children":[{"content":"<strong>Classification Metrics (When to Use)</strong>:","children":[{"content":"<strong>Accuracy</strong>: For balanced datasets where error costs are similar.","children":[],"payload":{"tag":"li","lines":"323,324"}},{"content":"<strong>Precision</strong>: For imbalanced datasets with a high cost of false positives (e.g., fraud/spam detection).","children":[],"payload":{"tag":"li","lines":"324,325"}},{"content":"<strong>Recall</strong>: For imbalanced datasets with a high cost of false negatives (e.g., disease detection).","children":[],"payload":{"tag":"li","lines":"325,326"}},{"content":"<strong>F1 Score</strong>: To balance both precision and recall (e.g., text classification).","children":[],"payload":{"tag":"li","lines":"326,327"}},{"content":"<strong>AUC Curve</strong>: For binary classification to evaluate a model&apos;s discrimination ability.","children":[],"payload":{"tag":"li","lines":"327,328"}}],"payload":{"tag":"li","lines":"322,328"}},{"content":"<strong>Regression Metrics (When to Use)</strong>:","children":[{"content":"<strong>Mean Absolute Error (MAE)</strong>: When data has outliers and you don&apos;t want them to have a disproportionate effect.","children":[],"payload":{"tag":"li","lines":"329,330"}},{"content":"<strong>Mean Squared Error (MSE)</strong>: When large errors are more problematic than small ones.","children":[],"payload":{"tag":"li","lines":"330,331"}},{"content":"<strong>RMSE</strong>: Like MSE but in the same units as the target variable; used when large errors are significantly more problematic.","children":[],"payload":{"tag":"li","lines":"331,332"}},{"content":"<strong>MAPE</strong>: To express errors as a percentage.","children":[],"payload":{"tag":"li","lines":"332,334"}}],"payload":{"tag":"li","lines":"328,334"}}],"payload":{"tag":"h3","lines":"321,322"}},{"content":"2. Interpret Confusion Matrices","children":[{"content":"<strong>Purpose</strong>: Not a metric itself, but forms the basis for many other performance metrics.","children":[],"payload":{"tag":"li","lines":"335,336"}},{"content":"<strong>Components</strong>:","children":[{"content":"<strong>True Positive (TP)</strong>: Model correctly predicted the positive class.","children":[],"payload":{"tag":"li","lines":"337,338"}},{"content":"<strong>True Negative (TN)</strong>: Model correctly predicted the negative class.","children":[],"payload":{"tag":"li","lines":"338,339"}},{"content":"<strong>False Positive (FP) / Type 1 Error</strong>: Model incorrectly predicted the positive class.","children":[],"payload":{"tag":"li","lines":"339,340"}},{"content":"<strong>False Negative (FN) / Type 2 Error</strong>: Model incorrectly predicted the negative class.","children":[],"payload":{"tag":"li","lines":"340,342"}}],"payload":{"tag":"li","lines":"336,342"}}],"payload":{"tag":"h3","lines":"334,335"}},{"content":"3. Perform Online and Offline Model Evaluation","children":[{"content":"<strong>Online Evaluation</strong>:","children":[{"content":"<strong>Concept</strong>: Continuously assessing a model&apos;s performance using live data in production.","children":[],"payload":{"tag":"li","lines":"344,345"}},{"content":"<strong>Performance Metrics</strong>: Latency, Throughput, Data Drift.","children":[],"payload":{"tag":"li","lines":"345,346"}},{"content":"<strong>Business Metrics</strong>: Clickthrough Rate, Conversion Rate.","children":[],"payload":{"tag":"li","lines":"346,347"}},{"content":"<strong>A/B Testing</strong>: Evaluating multiple model versions by running them in parallel and distributing traffic.","children":[],"payload":{"tag":"li","lines":"347,349"}}],"payload":{"tag":"li","lines":"343,349"}}],"payload":{"tag":"h3","lines":"342,343"}},{"content":"4. Compare ML Models","children":[{"content":"<strong>Beyond standard metrics</strong>, consider computational complexity.","children":[],"payload":{"tag":"li","lines":"350,351"}},{"content":"<strong>Computational Complexity Metrics</strong>:","children":[{"content":"<strong>Time Complexity</strong>: Time taken by an algorithm for a given input size.","children":[],"payload":{"tag":"li","lines":"352,353"}},{"content":"<strong>Space Complexity</strong>: Amount of additional memory an algorithm needs.","children":[],"payload":{"tag":"li","lines":"353,354"}},{"content":"<strong>Sample Complexity</strong>: Number of training samples needed to achieve desired performance.","children":[],"payload":{"tag":"li","lines":"354,355"}},{"content":"<strong>Parametricity</strong>: Whether a model has a fixed or dynamic number of parameters.","children":[],"payload":{"tag":"li","lines":"355,356"}}],"payload":{"tag":"li","lines":"351,356"}}],"payload":{"tag":"h3","lines":"349,350"}}],"payload":{"tag":"h2","lines":"316,317"}}],"payload":{"tag":"h1","lines":"8,9"}},{"colorFreezeLevel":2,"initialExpandLevel":4,"maxWidth":350})</script>
</body>
</html>
