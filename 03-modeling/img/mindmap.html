<!doctype html>
<html>
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta http-equiv="X-UA-Compatible" content="ie=edge" />
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
html {
  font-family: ui-sans-serif, system-ui, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.markmap-dark {
  background: #27272a;
  color: white;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.18.12/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/index.js"></script><script>((r) => {
          setTimeout(r);
        })(() => {
  const { markmap, mm } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute("style", "position:absolute;bottom:20px;right:20px");
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root2, jsonOptions) => {
              const markmap = getMarkmap();
              window.mm = markmap.Markmap.create(
                "svg#mindmap",
                (getOptions || markmap.deriveOptions)(jsonOptions),
                root2
              );
              if (window.matchMedia("(prefers-color-scheme: dark)").matches) {
                document.documentElement.classList.add("markmap-dark");
              }
            })(() => window.markmap,null,{"content":"Certification Exam Refresher","children":[{"content":"Task Statement 3.1: ML Problem Framing","children":[{"content":"1. Determine When to Use and When Not to Use ML","children":[{"content":"When ML Might Be a Good Fit","children":[{"content":"<strong>Requires Significant Data</strong>: ML needs large datasets to build a predictable model.","children":[],"payload":{"tag":"li","lines":"15,16"}},{"content":"<strong>Requires Expertise</strong>: Needs data processing and feature engineering to handle noise.","children":[],"payload":{"tag":"li","lines":"16,17"}},{"content":"<strong>Requires Powerful Machines</strong>: Needs computational power to crunch data.","children":[],"payload":{"tag":"li","lines":"17,19"}}],"payload":{"tag":"h4","lines":"14,15"}},{"content":"When ML is NOT the Right Solution","children":[{"content":"<strong>Mission-Critical Applications</strong>: Scenarios where prediction errors are unacceptable.","children":[],"payload":{"tag":"li","lines":"20,21"}},{"content":"<strong>Simple Problems</strong>: Can be solved with simple rules and traditional programming (e.g., rule engines).","children":[],"payload":{"tag":"li","lines":"21,22"}},{"content":"<strong>Be Cognizant</strong>: ML is an expensive solution; ensure it&apos;s the right choice.","children":[],"payload":{"tag":"li","lines":"22,24"}}],"payload":{"tag":"h4","lines":"19,20"}}],"payload":{"tag":"h3","lines":"12,13"}},{"content":"2. Supervised vs. Unsupervised Learning","children":[{"content":"Supervised Learning","children":[{"content":"<strong>Data</strong>: Uses labeled training data.","children":[],"payload":{"tag":"li","lines":"27,28"}},{"content":"<strong>Features</strong>: Dependent and independent features are clearly defined.","children":[],"payload":{"tag":"li","lines":"28,29"}},{"content":"<strong>Primary Task</strong>: Predict or classify new observations based on learned patterns.","children":[],"payload":{"tag":"li","lines":"29,30"}},{"content":"<strong>Feedback</strong>: Possible, as the difference between actual and predicted values can be computed.","children":[],"payload":{"tag":"li","lines":"30,31"}},{"content":"<strong>Common Techniques</strong>:","children":[{"content":"Linear Regression","children":[],"payload":{"tag":"li","lines":"32,33"}},{"content":"Logistic Regression","children":[],"payload":{"tag":"li","lines":"33,34"}},{"content":"Time Series Forecasting","children":[],"payload":{"tag":"li","lines":"34,36"}}],"payload":{"tag":"li","lines":"31,36"}}],"payload":{"tag":"h4","lines":"26,27"}},{"content":"Unsupervised Learning","children":[{"content":"<strong>Data</strong>: Deals with unlabeled data.","children":[],"payload":{"tag":"li","lines":"37,38"}},{"content":"<strong>Features</strong>: Target feature is not available.","children":[],"payload":{"tag":"li","lines":"38,39"}},{"content":"<strong>Primary Task</strong>: Uncover hidden structures or patterns without explicit guidance.","children":[],"payload":{"tag":"li","lines":"39,40"}},{"content":"<strong>Feedback</strong>: Not possible.","children":[],"payload":{"tag":"li","lines":"40,41"}},{"content":"<strong>Common Techniques</strong>:","children":[{"content":"Clustering","children":[],"payload":{"tag":"li","lines":"42,43"}},{"content":"Association Learning","children":[],"payload":{"tag":"li","lines":"43,44"}},{"content":"Dimensionality Reduction","children":[],"payload":{"tag":"li","lines":"44,46"}}],"payload":{"tag":"li","lines":"41,46"}}],"payload":{"tag":"h4","lines":"36,37"}}],"payload":{"tag":"h3","lines":"24,25"}},{"content":"3. Select the Right Model Type","children":[{"content":"Classification","children":[{"content":"<strong>Use Case</strong>: When the dependent feature is categorical.","children":[],"payload":{"tag":"li","lines":"49,50"}},{"content":"<strong>Variations</strong>:","children":[{"content":"<strong>Binary Classification</strong>: Target has two outcomes (e.g., yes/no, true/false).","children":[],"payload":{"tag":"li","lines":"51,52"}},{"content":"<strong>Multiclass Classification</strong>","children":[],"payload":{"tag":"li","lines":"52,53"}},{"content":"<strong>Multilabel Classification</strong>","children":[],"payload":{"tag":"li","lines":"53,55"}}],"payload":{"tag":"li","lines":"50,55"}}],"payload":{"tag":"h4","lines":"48,49"}},{"content":"Regression","children":[{"content":"<strong>Use Case</strong>: When the target feature is quantitative or continuous.","children":[],"payload":{"tag":"li","lines":"56,57"}},{"content":"<strong>Variations</strong>:","children":[{"content":"Linear Regression","children":[],"payload":{"tag":"li","lines":"58,59"}},{"content":"Multiple Regression","children":[],"payload":{"tag":"li","lines":"59,60"}},{"content":"Polynomial Regression","children":[],"payload":{"tag":"li","lines":"60,62"}}],"payload":{"tag":"li","lines":"57,62"}}],"payload":{"tag":"h4","lines":"55,56"}},{"content":"Time Series Forecasting","children":[{"content":"<strong>Use Case</strong>: For data collected at regular time intervals.","children":[],"payload":{"tag":"li","lines":"63,64"}},{"content":"<strong>Core Components</strong>:","children":[{"content":"<strong>Trend</strong>: Directionality of data over time.","children":[],"payload":{"tag":"li","lines":"65,66"}},{"content":"<strong>Seasonality</strong>: Periodic fluctuations at regular intervals.","children":[],"payload":{"tag":"li","lines":"66,67"}},{"content":"<strong>Cyclical Variations</strong>: Non-repeating fluctuations at irregular intervals.","children":[],"payload":{"tag":"li","lines":"67,68"}},{"content":"<strong>Irregularity</strong>: Randomness or noise in the data.","children":[],"payload":{"tag":"li","lines":"68,70"}}],"payload":{"tag":"li","lines":"64,70"}}],"payload":{"tag":"h4","lines":"62,63"}},{"content":"Clustering","children":[{"content":"<strong>Type</strong>: Unsupervised learning algorithm.","children":[],"payload":{"tag":"li","lines":"71,72"}},{"content":"<strong>Goal</strong>: Group similar data points into clusters based on similarity (distance).","children":[],"payload":{"tag":"li","lines":"72,73"}},{"content":"<strong>Categories</strong>:","children":[{"content":"<strong>Centroid-based</strong>: Requires a predetermined number of clusters.","children":[],"payload":{"tag":"li","lines":"74,75"}},{"content":"<strong>Density-based</strong>: Does not require a predetermined number of clusters.","children":[],"payload":{"tag":"li","lines":"75,76"}},{"content":"<strong>Hierarchical</strong>: Builds a hierarchy of clusters based on similarities.","children":[],"payload":{"tag":"li","lines":"76,77"}},{"content":"<strong>Distribution-based</strong>: Assumes data is from a mixture of probability distributions.","children":[],"payload":{"tag":"li","lines":"77,79"}}],"payload":{"tag":"li","lines":"73,79"}}],"payload":{"tag":"h4","lines":"70,71"}},{"content":"Advanced Techniques: Deep Learning","children":[{"content":"<strong>Purpose</strong>: Addresses limitations of traditional ML, like handling large/complex data and non-linear relationships.","children":[],"payload":{"tag":"li","lines":"80,81"}},{"content":"<strong>Concept</strong>: A subset of ML modeled after the human brain, using neural networks with multiple layers.","children":[],"payload":{"tag":"li","lines":"81,82"}},{"content":"<strong>Techniques</strong>:","children":[{"content":"<strong>CNN (Convolutional Neural Networks)</strong>: Uses layers to extract features for classification/detection.","children":[],"payload":{"tag":"li","lines":"83,84"}},{"content":"<strong>RNN (Recurrent Neural Networks)</strong>: Designed to process sequential data using internal memory.","children":[],"payload":{"tag":"li","lines":"84,85"}},{"content":"<strong>Transfer Learning</strong>: Leverages knowledge from a pre-trained model on one task to improve performance on a new, related task.","children":[],"payload":{"tag":"li","lines":"85,87"}}],"payload":{"tag":"li","lines":"82,87"}}],"payload":{"tag":"h4","lines":"79,80"}}],"payload":{"tag":"h3","lines":"46,47"}}],"payload":{"tag":"h2","lines":"10,11"}},{"content":"Task Statement 3.2: Amazon SageMaker Built-in Algorithms","children":[{"content":"Guiding Principle","children":[{"content":"The data type in the question will help narrow down your algorithm choices.","children":[],"payload":{"tag":"li","lines":"92,94"}}],"payload":{"tag":"h3","lines":"91,92"}},{"content":"1. Algorithms for Tabular Data","children":[{"content":"<strong>Definition</strong>: Datasets organized in tables (rows/observations, columns/features).","children":[],"payload":{"tag":"li","lines":"95,96"}},{"content":"<strong>Algorithms</strong>:","children":[{"content":"XGBoost","children":[],"payload":{"tag":"li","lines":"97,98"}},{"content":"Linear Learner","children":[],"payload":{"tag":"li","lines":"98,99"}},{"content":"K-Nearest Neighbor (KNN)","children":[],"payload":{"tag":"li","lines":"99,100"}},{"content":"Factorization Machines","children":[],"payload":{"tag":"li","lines":"100,102"}}],"payload":{"tag":"li","lines":"96,102"}}],"payload":{"tag":"h3","lines":"94,95"}},{"content":"2. Algorithms for Time Series Data","children":[{"content":"<strong>Definition</strong>: Data recorded over consistent time intervals.","children":[],"payload":{"tag":"li","lines":"103,104"}},{"content":"<strong>Examples</strong>: Forecasting product demand, analyzing server loads.","children":[],"payload":{"tag":"li","lines":"104,105"}},{"content":"<strong>Algorithm</strong>:","children":[{"content":"DeepAR","children":[],"payload":{"tag":"li","lines":"106,108"}}],"payload":{"tag":"li","lines":"105,108"}}],"payload":{"tag":"h3","lines":"102,103"}},{"content":"3. Algorithms for Unsupervised Learning","children":[{"content":"<strong>Use Cases</strong>: Unlabeled data for clustering, dimensionality reduction, anomaly detection.","children":[],"payload":{"tag":"li","lines":"109,110"}},{"content":"<strong>Algorithms</strong>:","children":[{"content":"Principal Component Analysis (PCA)","children":[],"payload":{"tag":"li","lines":"111,112"}},{"content":"Random Cut Forest","children":[],"payload":{"tag":"li","lines":"112,113"}},{"content":"IP Insights","children":[],"payload":{"tag":"li","lines":"113,114"}},{"content":"K-Means","children":[],"payload":{"tag":"li","lines":"114,116"}}],"payload":{"tag":"li","lines":"110,116"}}],"payload":{"tag":"h3","lines":"108,109"}},{"content":"4. Algorithms for Text Data (NLP)","children":[{"content":"<strong>Use Cases</strong>: Document summarization, topic modeling, language translation.","children":[],"payload":{"tag":"li","lines":"117,118"}},{"content":"<strong>Algorithms</strong>:","children":[{"content":"Object2Vec","children":[],"payload":{"tag":"li","lines":"119,120"}},{"content":"Latent Dirichlet Allocation (LDA)","children":[],"payload":{"tag":"li","lines":"120,121"}},{"content":"Neural Topic Model (NTM)","children":[],"payload":{"tag":"li","lines":"121,122"}},{"content":"BlazingText","children":[],"payload":{"tag":"li","lines":"122,123"}},{"content":"Sequence-to-Sequence","children":[],"payload":{"tag":"li","lines":"123,125"}}],"payload":{"tag":"li","lines":"118,125"}}],"payload":{"tag":"h3","lines":"116,117"}},{"content":"5. Algorithms for Image Data","children":[{"content":"<strong>Use Cases</strong>: Analyze and process image data.","children":[],"payload":{"tag":"li","lines":"126,127"}},{"content":"<strong>Algorithms</strong>:","children":[{"content":"Image Classification","children":[],"payload":{"tag":"li","lines":"128,129"}},{"content":"Object Detection","children":[],"payload":{"tag":"li","lines":"129,130"}},{"content":"Semantic Segmentation","children":[],"payload":{"tag":"li","lines":"130,132"}}],"payload":{"tag":"li","lines":"127,132"}}],"payload":{"tag":"h3","lines":"125,126"}}],"payload":{"tag":"h2","lines":"89,90"}},{"content":"Task Statement 3.3: Model Training &amp; Optimization","children":[{"content":"1. Splitting Data","children":[{"content":"<strong>Training Data</strong>: Used to train the model.","children":[],"payload":{"tag":"li","lines":"137,138"}},{"content":"<strong>Validation Data (Optional)</strong>: Measures model performance during training and tunes hyperparameters.","children":[],"payload":{"tag":"li","lines":"138,139"}},{"content":"<strong>Testing Data</strong>: Determines how well the model generalizes to unseen data.","children":[],"payload":{"tag":"li","lines":"139,140"}},{"content":"<strong>Cross-Validation</strong>: Process of validating the model against fresh, unseen data.","children":[{"content":"<strong>Techniques</strong>:","children":[{"content":"K-fold Cross-Validation","children":[],"payload":{"tag":"li","lines":"142,143"}},{"content":"Stratified K-fold Cross-Validation","children":[],"payload":{"tag":"li","lines":"143,144"}},{"content":"Leave-one-out Cross-Validation","children":[],"payload":{"tag":"li","lines":"144,146"}}],"payload":{"tag":"li","lines":"141,146"}}],"payload":{"tag":"li","lines":"140,146"}}],"payload":{"tag":"h3","lines":"136,137"}},{"content":"2. Optimization Techniques","children":[{"content":"<strong>Loss Function</strong>: Measures model accuracy (difference between predicted and actual output).","children":[],"payload":{"tag":"li","lines":"147,148"}},{"content":"<strong>Optimization Goal</strong>: Minimize the loss function for quick model convergence.","children":[],"payload":{"tag":"li","lines":"148,149"}},{"content":"<strong>Gradient Descent</strong>: A common optimization technique.","children":[{"content":"<strong>Challenge</strong>: May get stuck in local minima.","children":[],"payload":{"tag":"li","lines":"150,151"}},{"content":"<strong>Solutions</strong>:","children":[{"content":"Stochastic Gradient Descent","children":[],"payload":{"tag":"li","lines":"152,153"}},{"content":"Batch Gradient Descent","children":[],"payload":{"tag":"li","lines":"153,155"}}],"payload":{"tag":"li","lines":"151,155"}}],"payload":{"tag":"li","lines":"149,155"}}],"payload":{"tag":"h3","lines":"146,147"}},{"content":"3. Choosing Compute Resources","children":[{"content":"<strong>CPUs are good for</strong>:","children":[{"content":"Simpler classification/regression problems.","children":[],"payload":{"tag":"li","lines":"157,158"}},{"content":"Smaller models with fewer parameters.","children":[],"payload":{"tag":"li","lines":"158,159"}},{"content":"Low latency requirements.","children":[],"payload":{"tag":"li","lines":"159,160"}},{"content":"Severe budget constraints.","children":[],"payload":{"tag":"li","lines":"160,161"}}],"payload":{"tag":"li","lines":"156,161"}},{"content":"<strong>GPUs are good for</strong>:","children":[{"content":"Complex models (Deep Learning).","children":[],"payload":{"tag":"li","lines":"162,163"}},{"content":"Models with a large number of parameters.","children":[],"payload":{"tag":"li","lines":"163,164"}},{"content":"High throughput requirements.","children":[],"payload":{"tag":"li","lines":"164,165"}},{"content":"Significant performance requirements.","children":[],"payload":{"tag":"li","lines":"165,166"}}],"payload":{"tag":"li","lines":"161,166"}},{"content":"<strong>Distributed Training</strong>: For complex models on large datasets across multiple instances.","children":[{"content":"<strong>SageMaker Strategies</strong>: Data Parallelism &amp; Model Parallelism.","children":[],"payload":{"tag":"li","lines":"167,168"}},{"content":"<strong>SageMaker Tools</strong>: Prebuilt Docker images with Apache Spark.","children":[],"payload":{"tag":"li","lines":"168,170"}}],"payload":{"tag":"li","lines":"166,170"}}],"payload":{"tag":"h3","lines":"155,156"}},{"content":"4. Updating &amp; Retraining Models","children":[{"content":"<strong>Importance</strong>: Retraining with the latest data keeps models up-to-date and accurate.","children":[],"payload":{"tag":"li","lines":"171,172"}},{"content":"<strong>Amazon SageMaker Canvas</strong>:","children":[{"content":"Drag-and-drop UI for non-technical users.","children":[],"payload":{"tag":"li","lines":"173,174"}},{"content":"Offers features for manual or automatically scheduled model updates.","children":[],"payload":{"tag":"li","lines":"174,176"}}],"payload":{"tag":"li","lines":"172,176"}}],"payload":{"tag":"h3","lines":"170,171"}}],"payload":{"tag":"h2","lines":"134,135"}},{"content":"Task Statement 3.4: Hyperparameter Tuning &amp; Model Concepts","children":[{"content":"1. Regularization","children":[{"content":"<strong>Goal</strong>: Prevent overfitting and improve model performance.","children":[],"payload":{"tag":"li","lines":"181,182"}},{"content":"<strong>Overfitting</strong>: Performs well on training data, poorly on new data.","children":[],"payload":{"tag":"li","lines":"182,183"}},{"content":"<strong>Underfitting</strong>: Unable to learn hidden patterns in the data.","children":[],"payload":{"tag":"li","lines":"183,184"}},{"content":"<strong>Techniques</strong>:","children":[{"content":"<strong>L1 Regularization</strong>: Adds sum of <em>absolute</em> values of coefficients to the loss function. Good for minimizing impact of irrelevant features.","children":[],"payload":{"tag":"li","lines":"185,186"}},{"content":"<strong>L2 Regularization</strong>: Adds sum of <em>squared</em> values of coefficients to the loss function. Distributes the impact of all important features.","children":[],"payload":{"tag":"li","lines":"186,187"}},{"content":"<strong>Early Stopping</strong>: Stops training when performance on a validation set stops improving.","children":[],"payload":{"tag":"li","lines":"187,189"}}],"payload":{"tag":"li","lines":"184,189"}}],"payload":{"tag":"h3","lines":"180,181"}},{"content":"2. Cross-Validation","children":[{"content":"<strong>Goal</strong>: Prevent overfitting by assessing performance and generalizability, especially with limited data.","children":[],"payload":{"tag":"li","lines":"190,191"}},{"content":"<strong>Concept</strong>: Partitions the dataset into training and testing subsets to train on different parts of the data.","children":[],"payload":{"tag":"li","lines":"191,192"}},{"content":"<strong>Techniques</strong>:","children":[{"content":"<strong>K-fold</strong>: Dataset split into K folds. Train on K-1, test on the Kth fold. Repeat K times.","children":[],"payload":{"tag":"li","lines":"193,194"}},{"content":"<strong>Stratified K-fold</strong>: Like K-fold, but each fold maintains the same class distribution as the original dataset. Effective for imbalanced data.","children":[],"payload":{"tag":"li","lines":"194,195"}},{"content":"<strong>Time Series</strong>: Folds are created sequentially based on time, ensuring chronological order. Effective when data order is important.","children":[],"payload":{"tag":"li","lines":"195,197"}}],"payload":{"tag":"li","lines":"192,197"}}],"payload":{"tag":"h3","lines":"189,190"}},{"content":"3. Initializing Models for Tuning","children":[{"content":"<strong>Process</strong>: Initialize algorithms with a starting set of hyperparameter values.","children":[],"payload":{"tag":"li","lines":"198,199"}},{"content":"<strong>Ranges</strong>: Tuning jobs search for the best values over defined ranges.","children":[],"payload":{"tag":"li","lines":"199,200"}},{"content":"<strong>Range Types</strong>:","children":[{"content":"Categorical Parameter","children":[],"payload":{"tag":"li","lines":"201,202"}},{"content":"Continuous Parameter","children":[],"payload":{"tag":"li","lines":"202,203"}},{"content":"Integer Parameter","children":[],"payload":{"tag":"li","lines":"203,205"}}],"payload":{"tag":"li","lines":"200,205"}}],"payload":{"tag":"h3","lines":"197,198"}},{"content":"4. Neural Network Architecture","children":[{"content":"<strong>Components</strong>:","children":[{"content":"<strong>Input Layer</strong>: Receives data.","children":[],"payload":{"tag":"li","lines":"207,208"}},{"content":"<strong>Hidden Layer(s)</strong>: Transform input data.","children":[],"payload":{"tag":"li","lines":"208,209"}},{"content":"<strong>Output Layer</strong>: Makes final predictions.","children":[],"payload":{"tag":"li","lines":"209,210"}}],"payload":{"tag":"li","lines":"206,210"}},{"content":"<strong>Key Concepts</strong>:","children":[{"content":"<strong>Weights</strong>: Determine feature importance; adjusted during training.","children":[],"payload":{"tag":"li","lines":"211,212"}},{"content":"<strong>Biases</strong>: One per neuron; helps the model fit the data better.","children":[],"payload":{"tag":"li","lines":"212,213"}},{"content":"<strong>Activation Function</strong>: Introduces non-linearity to learn complex patterns.","children":[{"content":"<em>Examples</em>: Sigmoid, Tanh, ReLU, Softmax.","children":[],"payload":{"tag":"li","lines":"214,216"}}],"payload":{"tag":"li","lines":"213,216"}}],"payload":{"tag":"li","lines":"210,216"}}],"payload":{"tag":"h3","lines":"205,206"}},{"content":"5. Understanding Tree-Based Models","children":[{"content":"<strong>Type</strong>: Supervised learning algorithm that builds a tree structure.","children":[],"payload":{"tag":"li","lines":"217,218"}},{"content":"<strong>Structure</strong>:","children":[{"content":"<strong>Root Node</strong>: Top node, represents the entire dataset.","children":[],"payload":{"tag":"li","lines":"219,220"}},{"content":"<strong>Sub-nodes</strong>: Intermediate decision points that split the data.","children":[],"payload":{"tag":"li","lines":"220,221"}},{"content":"<strong>Branches/Edges</strong>: Represent the outcomes of a decision.","children":[],"payload":{"tag":"li","lines":"221,222"}},{"content":"<strong>Leaf Nodes</strong>: Terminal nodes representing the final outcome.","children":[],"payload":{"tag":"li","lines":"222,223"}}],"payload":{"tag":"li","lines":"218,223"}},{"content":"<strong>Key Hyperparameters</strong>:","children":[{"content":"<code>no_of_folds</code>","children":[],"payload":{"tag":"li","lines":"224,225"}},{"content":"<code>max_depth</code>","children":[],"payload":{"tag":"li","lines":"225,226"}},{"content":"<code>min_samples_split</code>","children":[],"payload":{"tag":"li","lines":"226,227"}},{"content":"<code>min_samples_leaf</code>","children":[],"payload":{"tag":"li","lines":"227,229"}}],"payload":{"tag":"li","lines":"223,229"}}],"payload":{"tag":"h3","lines":"216,217"}},{"content":"6. Understanding Linear Models","children":[{"content":"<strong>Gradient Descent</strong>: Advanced algorithm to find optimal hyperparameter values.","children":[],"payload":{"tag":"li","lines":"230,231"}},{"content":"<strong>Key Hyperparameters</strong>:","children":[{"content":"<strong>Learning Rate</strong>: Critical for the efficiency of the optimization process.","children":[],"payload":{"tag":"li","lines":"232,233"}},{"content":"<strong>Alpha</strong>: Controls regularization strength in Ridge Regression to balance bias and variance.","children":[],"payload":{"tag":"li","lines":"233,235"}}],"payload":{"tag":"li","lines":"231,235"}}],"payload":{"tag":"h3","lines":"229,230"}}],"payload":{"tag":"h2","lines":"178,179"}},{"content":"Task Statement 3.5: Model Evaluation","children":[{"content":"Note on Overlapping Topics","children":[{"content":"Overfitting, underfitting, and cross-validation are covered in detail under <strong>Task Statement 3.4</strong>.","children":[],"payload":{"tag":"li","lines":"240,242"}}],"payload":{"tag":"h3","lines":"239,240"}},{"content":"1. Evaluate Metrics","children":[{"content":"<strong>Classification Metrics (When to Use)</strong>:","children":[{"content":"<strong>Accuracy</strong>: For balanced datasets where error costs are similar.","children":[],"payload":{"tag":"li","lines":"244,245"}},{"content":"<strong>Precision</strong>: For imbalanced datasets with a high cost of false positives (e.g., fraud/spam detection).","children":[],"payload":{"tag":"li","lines":"245,246"}},{"content":"<strong>Recall</strong>: For imbalanced datasets with a high cost of false negatives (e.g., disease detection).","children":[],"payload":{"tag":"li","lines":"246,247"}},{"content":"<strong>F1 Score</strong>: To balance both precision and recall (e.g., text classification).","children":[],"payload":{"tag":"li","lines":"247,248"}},{"content":"<strong>AUC Curve</strong>: For binary classification to evaluate a model&apos;s discrimination ability.","children":[],"payload":{"tag":"li","lines":"248,249"}}],"payload":{"tag":"li","lines":"243,249"}},{"content":"<strong>Regression Metrics (When to Use)</strong>:","children":[{"content":"<strong>Mean Absolute Error (MAE)</strong>: When data has outliers and you don&apos;t want them to have a disproportionate effect.","children":[],"payload":{"tag":"li","lines":"250,251"}},{"content":"<strong>Mean Squared Error (MSE)</strong>: When large errors are more problematic than small ones.","children":[],"payload":{"tag":"li","lines":"251,252"}},{"content":"<strong>RMSE</strong>: Like MSE but in the same units as the target variable; used when large errors are significantly more problematic.","children":[],"payload":{"tag":"li","lines":"252,253"}},{"content":"<strong>MAPE</strong>: To express errors as a percentage.","children":[],"payload":{"tag":"li","lines":"253,255"}}],"payload":{"tag":"li","lines":"249,255"}}],"payload":{"tag":"h3","lines":"242,243"}},{"content":"2. Interpret Confusion Matrices","children":[{"content":"<strong>Purpose</strong>: Not a metric itself, but forms the basis for many other performance metrics.","children":[],"payload":{"tag":"li","lines":"256,257"}},{"content":"<strong>Components</strong>:","children":[{"content":"<strong>True Positive (TP)</strong>: Model correctly predicted the positive class.","children":[],"payload":{"tag":"li","lines":"258,259"}},{"content":"<strong>True Negative (TN)</strong>: Model correctly predicted the negative class.","children":[],"payload":{"tag":"li","lines":"259,260"}},{"content":"<strong>False Positive (FP) / Type 1 Error</strong>: Model incorrectly predicted the positive class.","children":[],"payload":{"tag":"li","lines":"260,261"}},{"content":"<strong>False Negative (FN) / Type 2 Error</strong>: Model incorrectly predicted the negative class.","children":[],"payload":{"tag":"li","lines":"261,263"}}],"payload":{"tag":"li","lines":"257,263"}}],"payload":{"tag":"h3","lines":"255,256"}},{"content":"3. Perform Online and Offline Model Evaluation","children":[{"content":"<strong>Online Evaluation</strong>:","children":[{"content":"<strong>Concept</strong>: Continuously assessing a model&apos;s performance using live data in production.","children":[],"payload":{"tag":"li","lines":"265,266"}},{"content":"<strong>Performance Metrics</strong>: Latency, Throughput, Data Drift.","children":[],"payload":{"tag":"li","lines":"266,267"}},{"content":"<strong>Business Metrics</strong>: Clickthrough Rate, Conversion Rate.","children":[],"payload":{"tag":"li","lines":"267,268"}},{"content":"<strong>A/B Testing</strong>: Evaluating multiple model versions by running them in parallel and distributing traffic.","children":[],"payload":{"tag":"li","lines":"268,270"}}],"payload":{"tag":"li","lines":"264,270"}}],"payload":{"tag":"h3","lines":"263,264"}},{"content":"4. Compare ML Models","children":[{"content":"<strong>Beyond standard metrics</strong>, consider computational complexity.","children":[],"payload":{"tag":"li","lines":"271,272"}},{"content":"<strong>Computational Complexity Metrics</strong>:","children":[{"content":"<strong>Time Complexity</strong>: Time taken by an algorithm for a given input size.","children":[],"payload":{"tag":"li","lines":"273,274"}},{"content":"<strong>Space Complexity</strong>: Amount of additional memory an algorithm needs.","children":[],"payload":{"tag":"li","lines":"274,275"}},{"content":"<strong>Sample Complexity</strong>: Number of training samples needed to achieve desired performance.","children":[],"payload":{"tag":"li","lines":"275,276"}},{"content":"<strong>Parametricity</strong>: Whether a model has a fixed or dynamic number of parameters.","children":[],"payload":{"tag":"li","lines":"276,277"}}],"payload":{"tag":"li","lines":"272,277"}}],"payload":{"tag":"h3","lines":"270,271"}}],"payload":{"tag":"h2","lines":"237,238"}}],"payload":{"tag":"h1","lines":"8,9"}},{"colorFreezeLevel":2,"initialExpandLevel":4,"maxWidth":300})</script>
</body>
</html>
