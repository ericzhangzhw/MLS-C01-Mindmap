<!doctype html>
<html>
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta http-equiv="X-UA-Compatible" content="ie=edge" />
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
html {
  font-family: ui-sans-serif, system-ui, sans-serif, 'Apple Color Emoji',
    'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
.markmap-dark {
  background: #27272a;
  color: white;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.18.12/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.18.12/dist/index.js"></script><script>((r) => {
          setTimeout(r);
        })(() => {
  const { markmap, mm } = window;
  const toolbar = new markmap.Toolbar();
  toolbar.attach(mm);
  const el = toolbar.render();
  el.setAttribute("style", "position:absolute;bottom:20px;right:20px");
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root2, jsonOptions) => {
              const markmap = getMarkmap();
              window.mm = markmap.Markmap.create(
                "svg#mindmap",
                (getOptions || markmap.deriveOptions)(jsonOptions),
                root2
              );
              if (window.matchMedia("(prefers-color-scheme: dark)").matches) {
                document.documentElement.classList.add("markmap-dark");
              }
            })(() => window.markmap,null,{"content":"Domain 3: Modeling (36% of Exam)","children":[{"content":"Task Statement 3.1: ML Problem Framing","children":[{"content":"1. The ML Pipeline Context","children":[{"content":"<strong>Previous Step (Domain 2)</strong>: Data Preprocessing &amp; Visualization","children":[],"payload":{"tag":"li","lines":"13,14"}},{"content":"<strong>Current Step (This Course)</strong>: Building, Training &amp; Testing the ML Model","children":[],"payload":{"tag":"li","lines":"14,15"}},{"content":"<strong>Exam Focus</strong>: Choosing the right algorithm for a business use case and knowing its performance metrics.","children":[],"payload":{"tag":"li","lines":"15,17"}}],"payload":{"tag":"h3","lines":"12,13"}},{"content":"2. When to Use and Not to Use ML","children":[{"content":"When ML Might Be a Good Fit","children":[{"content":"<strong>Requires Significant Data</strong>: ML needs large datasets to build a predictable model.","children":[],"payload":{"tag":"li","lines":"20,21"}},{"content":"<strong>Requires Expertise</strong>: Needs data processing and feature engineering to handle noise and retain meaningful information.","children":[],"payload":{"tag":"li","lines":"21,22"}},{"content":"<strong>Requires Powerful &amp; Scalable Machines</strong>: Needs computational power to crunch data and scale as data grows.","children":[],"payload":{"tag":"li","lines":"22,24"}}],"payload":{"tag":"h4","lines":"19,20"}},{"content":"When ML is NOT the Right Solution","children":[{"content":"<strong>Mission-Critical Applications</strong>: Scenarios where prediction errors are unacceptable.","children":[],"payload":{"tag":"li","lines":"25,26"}},{"content":"<strong>Simple Problems</strong>: Can be solved with simple rules and traditional programming (e.g., rule engines).","children":[],"payload":{"tag":"li","lines":"26,27"}},{"content":"<strong>Be Cognizant</strong>: ML is an expensive solution; ensure it&apos;s the right choice.","children":[],"payload":{"tag":"li","lines":"27,29"}}],"payload":{"tag":"h4","lines":"24,25"}}],"payload":{"tag":"h3","lines":"17,18"}},{"content":"3. Identifying the Right Learning Type","children":[{"content":"Supervised Learning &#x1f468;&#x200d;&#x1f3eb;","children":[{"content":"<strong>Analogy</strong>: A child learning under the guidance of a teacher.","children":[],"payload":{"tag":"li","lines":"32,33"}},{"content":"<strong>Data</strong>: Uses <strong>labeled training data</strong> (the outcome is already known).","children":[],"payload":{"tag":"li","lines":"33,34"}},{"content":"<strong>Goal</strong>: Model the relationship between inputs and outputs to predict new outcomes.","children":[],"payload":{"tag":"li","lines":"34,36"}}],"payload":{"tag":"h4","lines":"31,32"}},{"content":"Unsupervised Learning &#x1f575;&#xfe0f;","children":[{"content":"<strong>Analogy</strong>: A child figuring things out without supervision.","children":[],"payload":{"tag":"li","lines":"37,38"}},{"content":"<strong>Data</strong>: Deals with <strong>unlabeled data</strong>.","children":[],"payload":{"tag":"li","lines":"38,39"}},{"content":"<strong>Goal</strong>: Discover hidden structures, patterns, or information on its own.","children":[],"payload":{"tag":"li","lines":"39,41"}}],"payload":{"tag":"h4","lines":"36,37"}},{"content":"Reinforcement Learning &#x1f916;","children":[{"content":"<strong>Analogy</strong>: Rewarding a kid for good behavior to reinforce it.","children":[],"payload":{"tag":"li","lines":"42,43"}},{"content":"<strong>Core Idea</strong>: An autonomous agent learns through trial and error in an interactive environment.","children":[],"payload":{"tag":"li","lines":"43,44"}},{"content":"<strong>Use Cases</strong>: Self-driving vehicles, robotics, dynamic pricing.","children":[],"payload":{"tag":"li","lines":"44,46"}}],"payload":{"tag":"h4","lines":"41,42"}}],"payload":{"tag":"h3","lines":"29,30"}},{"content":"4. Selecting the Right Model Type","children":[{"content":"Classification","children":[{"content":"<strong>Use Case</strong>: When the dependent feature is categorical (discrete classes).","children":[],"payload":{"tag":"li","lines":"49,50"}},{"content":"<strong>Types</strong>: Binary, Multiclass, Multilabel.","children":[],"payload":{"tag":"li","lines":"50,51"}},{"content":"<strong>Challenge</strong>: Imbalanced Data (Solved with <strong>SMOTE</strong>).","children":[],"payload":{"tag":"li","lines":"51,52"}},{"content":"<strong>Common Algorithms</strong>: Logistic Regression, Naive Bayes, SVM, KNN.","children":[],"payload":{"tag":"li","lines":"52,54"}}],"payload":{"tag":"h4","lines":"48,49"}},{"content":"Regression","children":[{"content":"<strong>Use Case</strong>: When the target feature is quantitative or continuous.","children":[],"payload":{"tag":"li","lines":"55,56"}},{"content":"<strong>Example</strong>: Predicting house prices.","children":[],"payload":{"tag":"li","lines":"56,57"}},{"content":"<strong>Variations</strong>: Linear, Multiple, Polynomial Regression.","children":[],"payload":{"tag":"li","lines":"57,59"}}],"payload":{"tag":"h4","lines":"54,55"}},{"content":"Time Series Forecasting &#x1f4c8;","children":[{"content":"<strong>Use Case</strong>: For data collected at regular time intervals.","children":[],"payload":{"tag":"li","lines":"60,61"}},{"content":"<strong>Key Feature</strong>: <strong>Time</strong> is always an independent feature.","children":[],"payload":{"tag":"li","lines":"61,62"}},{"content":"<strong>Core Components</strong>: Trend, Seasonality, Cyclical Variations, Irregularity.","children":[],"payload":{"tag":"li","lines":"62,64"}}],"payload":{"tag":"h4","lines":"59,60"}},{"content":"Clustering","children":[{"content":"<strong>Type</strong>: Unsupervised learning.","children":[],"payload":{"tag":"li","lines":"65,66"}},{"content":"<strong>Goal</strong>: Group similar data points into clusters.","children":[],"payload":{"tag":"li","lines":"66,67"}},{"content":"<strong>Categories</strong>: Centroid-based (K-Means), Density-based (DBScan), Hierarchical.","children":[],"payload":{"tag":"li","lines":"67,69"}}],"payload":{"tag":"h4","lines":"64,65"}},{"content":"Association Learning","children":[{"content":"<strong>Type</strong>: Unsupervised learning.","children":[],"payload":{"tag":"li","lines":"70,71"}},{"content":"<strong>Goal</strong>: Discover &quot;if-then&quot; relationships between features.","children":[],"payload":{"tag":"li","lines":"71,72"}},{"content":"<strong>Use Cases</strong>: Market basket analysis, recommendation systems.","children":[],"payload":{"tag":"li","lines":"72,74"}}],"payload":{"tag":"h4","lines":"69,70"}},{"content":"Advanced: Deep Learning &amp; Neural Networks &#x1f9e0;","children":[{"content":"<strong>Purpose</strong>: A subset of ML for large/complex data and non-linear relationships.","children":[],"payload":{"tag":"li","lines":"75,76"}},{"content":"<strong>CNN (Convolutional Neural Networks)</strong>: For image/video processing.","children":[],"payload":{"tag":"li","lines":"76,77"}},{"content":"<strong>RNN (Recurrent Neural Networks)</strong>: For sequential data (text, time series). Includes LSTM &amp; GRU.","children":[],"payload":{"tag":"li","lines":"77,78"}},{"content":"<strong>Transfer Learning</strong>: Reusing a pre-trained model on a new, related task to save resources and improve performance.","children":[],"payload":{"tag":"li","lines":"78,80"}}],"payload":{"tag":"h4","lines":"74,75"}}],"payload":{"tag":"h3","lines":"46,47"}}],"payload":{"tag":"h2","lines":"10,11"}},{"content":"Task Statement 3.2: Amazon SageMaker Built-in Algorithms","children":[{"content":"1. Introduction to the SageMaker Ecosystem","children":[{"content":"<strong>Overview</strong>: A fully managed service to prepare, build, train, and deploy ML models at scale.","children":[],"payload":{"tag":"li","lines":"85,86"}},{"content":"<strong>ML Lifecycle Services</strong>:","children":[{"content":"<strong>Data Collection</strong>: <strong>SageMaker Ground Truth</strong> for building highly accurate, labeled training sets.","children":[],"payload":{"tag":"li","lines":"87,88"}},{"content":"<strong>Data Analysis/Prep</strong>:","children":[{"content":"<strong>SageMaker Data Wrangler</strong>: Visualize and prepare data with no code.","children":[],"payload":{"tag":"li","lines":"89,90"}},{"content":"<strong>SageMaker Feature Store</strong>: Simplifies feature processing, storing, and retrieving for model development.","children":[],"payload":{"tag":"li","lines":"90,91"}}],"payload":{"tag":"li","lines":"88,91"}},{"content":"<strong>Model Building</strong>:","children":[{"content":"<strong>SageMaker Notebooks</strong>: Managed Jupyter Notebooks.","children":[],"payload":{"tag":"li","lines":"92,93"}},{"content":"<strong>SageMaker Studio</strong>: An IDE for the entire ML lifecycle.","children":[],"payload":{"tag":"li","lines":"93,94"}}],"payload":{"tag":"li","lines":"91,94"}},{"content":"<strong>Model Training</strong>:","children":[{"content":"<strong>Architecture</strong>:","children":[{"content":"Training data is stored in an <strong>S3 bucket</strong>.","children":[],"payload":{"tag":"li","lines":"96,97"}},{"content":"Training job runs on <strong>SageMaker compute instances</strong>.","children":[],"payload":{"tag":"li","lines":"97,98"}},{"content":"The training job (algorithm) is stored in <strong>Amazon ECR</strong>.","children":[],"payload":{"tag":"li","lines":"98,99"}},{"content":"Model output (artifacts) is stored in another <strong>S3 bucket</strong>.","children":[],"payload":{"tag":"li","lines":"99,100"}},{"content":"<em>Constraint</em>: Training data and job must be in the same AWS region.","children":[],"payload":{"tag":"li","lines":"100,101"}}],"payload":{"tag":"li","lines":"95,101"}},{"content":"<strong>Implementation Options</strong>:","children":[{"content":"<strong>Built-in Algorithms</strong>: Easiest option, requires no custom code.","children":[],"payload":{"tag":"li","lines":"102,103"}},{"content":"<strong>Script Mode</strong>: Use a custom Python script with supported frameworks (scikit-learn, TensorFlow, PyTorch).","children":[],"payload":{"tag":"li","lines":"103,104"}},{"content":"<strong>Custom Docker Image</strong>: For use cases not covered by the other options; requires Docker knowledge.","children":[],"payload":{"tag":"li","lines":"104,105"}}],"payload":{"tag":"li","lines":"101,105"}}],"payload":{"tag":"li","lines":"94,105"}},{"content":"<strong>Model Deployment</strong>:","children":[{"content":"<strong>SageMaker Hosting Services</strong>: For real-time inference with low latency.","children":[],"payload":{"tag":"li","lines":"106,107"}},{"content":"<strong>SageMaker Batch Transform</strong>: For asynchronous batch inference on large datasets.","children":[],"payload":{"tag":"li","lines":"107,108"}}],"payload":{"tag":"li","lines":"105,108"}},{"content":"<strong>Model Monitoring</strong>:","children":[{"content":"<strong>SageMaker Model Monitor</strong>: Continuously monitors deployed models for performance and concept drift.","children":[],"payload":{"tag":"li","lines":"109,111"}}],"payload":{"tag":"li","lines":"108,111"}}],"payload":{"tag":"li","lines":"86,111"}}],"payload":{"tag":"h3","lines":"84,85"}},{"content":"2. Algorithms for Tabular Data","children":[{"content":"<strong>XGBoost (Extreme Gradient Boosting)</strong>","children":[{"content":"<strong>Concept</strong>: A popular and efficient implementation of gradient-boosted decision trees.","children":[{"content":"<strong>Ensemble Learning</strong>: A &quot;wisdom of the crowd&quot; approach where multiple &quot;weak&quot; models are combined to create one strong model.","children":[],"payload":{"tag":"li","lines":"114,115"}},{"content":"<strong>Boosting</strong>: A sequential ensemble technique. Models are trained in sequence, with each new model focusing on correcting the errors of its predecessor. Models with larger errors are given higher weights.","children":[],"payload":{"tag":"li","lines":"115,116"}},{"content":"<strong>Gradient Boosting</strong>: A specific type of boosting that uses the gradient descent algorithm to minimize errors.","children":[],"payload":{"tag":"li","lines":"116,117"}},{"content":"<strong>Decision Tree</strong>: Predicts an outcome by evaluating a sequence of &quot;if-then-else&quot; questions on features, creating branches until a final decision (leaf) is reached.","children":[],"payload":{"tag":"li","lines":"117,118"}}],"payload":{"tag":"li","lines":"113,118"}},{"content":"<strong>Problem Type</strong>: Classification &amp; Regression.","children":[],"payload":{"tag":"li","lines":"118,119"}},{"content":"<strong>Data Formats</strong>: <code>libsvm</code>, <code>CSV</code>, <code>parquet</code>, <code>protobuf</code>.","children":[],"payload":{"tag":"li","lines":"119,120"}},{"content":"<strong>Compute</strong>: Supports CPU &amp; GPU.","children":[],"payload":{"tag":"li","lines":"120,121"}},{"content":"<strong>Required Hyperparameters</strong>: <code>num_round</code>, <code>num_class</code>.","children":[],"payload":{"tag":"li","lines":"121,122"}},{"content":"<strong>Metrics</strong>: MAE, MSE, RMSE (Regression); Accuracy, AUC, F1 Score (Classification).","children":[],"payload":{"tag":"li","lines":"122,123"}},{"content":"<strong>Use Cases</strong>: Fraud detection, stock price prediction, customer churn, sales forecasting, ad-click revenue.","children":[],"payload":{"tag":"li","lines":"123,124"}}],"payload":{"tag":"li","lines":"112,124"}},{"content":"<strong>Linear Learner</strong>","children":[{"content":"<strong>Concept</strong>: A supervised algorithm for classification or regression, great for large, high-dimensional datasets. It fits a line to data points by adjusting weights (m) and biases (b) for each feature (e.g., Y = m1x1 + m2x2 + ... + b). It uses <strong>stochastic gradient descent</strong> to iteratively adjust these parameters and minimize the difference between predicted and actual values.","children":[],"payload":{"tag":"li","lines":"125,126"}},{"content":"<strong>Problem Type</strong>: Classification &amp; Regression.","children":[],"payload":{"tag":"li","lines":"126,127"}},{"content":"<strong>Data Formats</strong>: <code>protobuf</code>, <code>CSV</code> for training. <code>JSON</code>, <code>protobuf</code>, <code>CSV</code> for inference.","children":[],"payload":{"tag":"li","lines":"127,128"}},{"content":"<strong>Compute</strong>: Supports CPU &amp; GPU.","children":[],"payload":{"tag":"li","lines":"128,129"}},{"content":"<strong>Required Hyperparameters</strong>: <code>num_classes</code>, <code>predictor_type</code>.","children":[],"payload":{"tag":"li","lines":"129,130"}},{"content":"<strong>Metrics</strong>: Cross-entropy loss, MAE, MSE (Regression); Precision, Recall, Accuracy (Classification).","children":[],"payload":{"tag":"li","lines":"130,131"}},{"content":"<strong>Use Cases</strong>: Loan application processing, email spam detection, recommendation systems.","children":[],"payload":{"tag":"li","lines":"131,132"}}],"payload":{"tag":"li","lines":"124,132"}},{"content":"<strong>K-Nearest Neighbor (KNN)</strong>","children":[{"content":"<strong>Concept</strong>: A non-parametric, index-based algorithm. It classifies a new data point based on the properties of its &quot;K&quot; closest neighbors.","children":[{"content":"<strong>For Regression</strong>: It averages the values of the K nearest neighbors (e.g., predicting a house price based on the prices of nearby, similar houses).","children":[],"payload":{"tag":"li","lines":"134,135"}},{"content":"<strong>For Classification</strong>: It uses a majority vote among the K nearest neighbors (e.g., predicting a house has 3 bedrooms because most houses in its community do).","children":[],"payload":{"tag":"li","lines":"135,136"}}],"payload":{"tag":"li","lines":"133,136"}},{"content":"<strong>Problem Type</strong>: Classification &amp; Regression.","children":[],"payload":{"tag":"li","lines":"136,137"}},{"content":"<strong>Data Formats</strong>: <code>protobuf</code>, <code>CSV</code> for training. <code>JSON</code>, <code>protobuf</code>, <code>CSV</code> for inference.","children":[],"payload":{"tag":"li","lines":"137,138"}},{"content":"<strong>Compute</strong>: Supports CPU &amp; GPU.","children":[],"payload":{"tag":"li","lines":"138,139"}},{"content":"<strong>Required Hyperparameters</strong>: <code>feature_dim</code>, <code>k</code>, <code>predictor_type</code>, <code>sample_size</code>.","children":[],"payload":{"tag":"li","lines":"139,140"}},{"content":"<strong>Metrics</strong>: MSE (Regression); Accuracy (Classification).","children":[],"payload":{"tag":"li","lines":"140,141"}},{"content":"<strong>Use Cases</strong>: Credit risk rating, recommendation systems, fraud detection.","children":[],"payload":{"tag":"li","lines":"141,142"}}],"payload":{"tag":"li","lines":"132,142"}},{"content":"<strong>Factorization Machines</strong>","children":[{"content":"<strong>Concept</strong>: An extension of a linear model designed to capture higher-order (pairwise) feature interactions in sparse datasets, like recommendation systems. It uses latent vectors (numerical representations of hidden features like genre or actors) to model these interactions.","children":[],"payload":{"tag":"li","lines":"143,144"}},{"content":"<strong>Problem Type</strong>: Binary Classification &amp; Regression.","children":[],"payload":{"tag":"li","lines":"144,145"}},{"content":"<strong>Limitations</strong>: Only considers pairwise features, does not support multiclass problems, does not support CSV format, performs poorly on dense data, and needs a lot of data (e.g., 10k-10M rows) to work around missing features.","children":[],"payload":{"tag":"li","lines":"145,146"}},{"content":"<strong>Data Formats</strong>: <code>protobuf</code> (float32 tensors) only for training. <code>JSON</code>, <code>protobuf</code> for inference.","children":[],"payload":{"tag":"li","lines":"146,147"}},{"content":"<strong>Compute</strong>: Recommended for CPU only.","children":[],"payload":{"tag":"li","lines":"147,148"}},{"content":"<strong>Required Hyperparameters</strong>: <code>feature_dim</code>, <code>num_factors</code>, <code>predictor_type</code>.","children":[],"payload":{"tag":"li","lines":"148,149"}},{"content":"<strong>Metrics</strong>: RMSE (Regression); Accuracy, Cross-entropy (Classification).","children":[],"payload":{"tag":"li","lines":"149,150"}},{"content":"<strong>Use Cases</strong>: Recommendation systems, ad-click prediction.","children":[],"payload":{"tag":"li","lines":"150,152"}}],"payload":{"tag":"li","lines":"142,152"}}],"payload":{"tag":"h3","lines":"111,112"}},{"content":"3. Algorithms for Time Series Data","children":[{"content":"<strong>DeepAR</strong>","children":[{"content":"<strong>Concept</strong>: A supervised algorithm using RNNs for forecasting one-dimensional time series data. It can learn from multiple related time series, which helps solve the &quot;cold start problem&quot; for new items that have no historical data.","children":[],"payload":{"tag":"li","lines":"154,155"}},{"content":"<strong>Forecast Types</strong>:","children":[{"content":"<strong>Point-in-time</strong>: Predicts a single value (e.g., we will sell 1000 units).","children":[],"payload":{"tag":"li","lines":"156,157"}},{"content":"<strong>Probabilistic</strong>: Predicts a range of values with a probability (e.g., we will sell 800-1200 units with 90% probability).","children":[],"payload":{"tag":"li","lines":"157,158"}}],"payload":{"tag":"li","lines":"155,158"}},{"content":"<strong>Data Format</strong>: <code>JSON Lines</code> format (can be GZIP or Parquet). Requires <code>start</code> (timestamp string) and <code>target</code> (array of values) fields. Optional fields include <code>dynamic_feat</code> (e.g., a boolean for a promotion) and <code>cat</code> (categorical features).","children":[],"payload":{"tag":"li","lines":"158,159"}},{"content":"<strong>Compute</strong>: Supports CPU &amp; GPU.","children":[],"payload":{"tag":"li","lines":"159,160"}},{"content":"<strong>Required Hyperparameters</strong>: <code>context_length</code>, <code>epochs</code>, <code>prediction_length</code>, <code>time_freq</code>.","children":[],"payload":{"tag":"li","lines":"160,161"}},{"content":"<strong>Metrics</strong>: RMSE, <code>wQuantileLoss</code>.","children":[],"payload":{"tag":"li","lines":"161,162"}},{"content":"<strong>Use Cases</strong>: Demand/sales forecasting, financial forecasting, risk assessment.","children":[],"payload":{"tag":"li","lines":"162,164"}}],"payload":{"tag":"li","lines":"153,164"}}],"payload":{"tag":"h3","lines":"152,153"}},{"content":"4. Algorithms for Unsupervised Learning","children":[{"content":"<strong>Principal Component Analysis (PCA)</strong>","children":[{"content":"<strong>Concept</strong>: Reduces the number of features (dimensionality) in a dataset by creating new, uncorrelated features called &quot;components&quot; that capture the most variance, without losing meaningful information.","children":[],"payload":{"tag":"li","lines":"166,167"}},{"content":"<strong>Analogy</strong>: Like taking photos of a 3D object from the best possible angles to capture all its important features in a 2D representation. These &quot;best angles&quot; are the principal components.","children":[],"payload":{"tag":"li","lines":"167,168"}},{"content":"<strong>Problem Type</strong>: Dimensionality Reduction.","children":[],"payload":{"tag":"li","lines":"168,169"}},{"content":"<strong>Modes</strong>: <code>regular</code> (for sparse data) and <code>randomized</code> (for large datasets).","children":[],"payload":{"tag":"li","lines":"169,170"}},{"content":"<strong>Data Formats</strong>: <code>CSV</code>, <code>protobuf</code> for training. <code>JSON</code> for inference.","children":[],"payload":{"tag":"li","lines":"170,171"}},{"content":"<strong>Compute</strong>: Supports CPU &amp; GPU.","children":[],"payload":{"tag":"li","lines":"171,172"}},{"content":"<strong>Required Hyperparameters</strong>: <code>feature_dim</code>, <code>mini_batch_size</code>, <code>num_components</code>.","children":[],"payload":{"tag":"li","lines":"172,173"}},{"content":"<strong>Use Cases</strong>: Image compression, financial analysis, customer feedback analysis.","children":[],"payload":{"tag":"li","lines":"173,174"}}],"payload":{"tag":"li","lines":"165,174"}},{"content":"<strong>Random Cut Forest (RCF)</strong>","children":[{"content":"<strong>Concept</strong>: Detects anomalies (outliers) by building an ensemble of trees. Data points that are easily isolated with fewer &quot;cuts&quot; are assigned a higher anomaly score.","children":[],"payload":{"tag":"li","lines":"175,176"}},{"content":"<strong>Analogy</strong>: A forester takes random sample plots (random cuts) in a forest (dataset). A tree (data point) that stands alone after a cut is considered isolated and thus an outlier.","children":[],"payload":{"tag":"li","lines":"176,177"}},{"content":"<strong>Problem Type</strong>: Anomaly Detection.","children":[],"payload":{"tag":"li","lines":"177,178"}},{"content":"<strong>Data Formats</strong>: <code>protobuf</code>, <code>CSV</code>.","children":[],"payload":{"tag":"li","lines":"178,179"}},{"content":"<strong>Compute</strong>: Recommended for CPU only.","children":[],"payload":{"tag":"li","lines":"179,180"}},{"content":"<strong>Required Hyperparameters</strong>: <code>feature_dim</code>.","children":[],"payload":{"tag":"li","lines":"180,181"}},{"content":"<strong>Metrics</strong>: F1 Score.","children":[],"payload":{"tag":"li","lines":"181,182"}},{"content":"<strong>Use Cases</strong>: Detecting fraud in financial transactions, identifying security breaches in network traffic, monitoring for bot activity in e-commerce.","children":[],"payload":{"tag":"li","lines":"182,183"}}],"payload":{"tag":"li","lines":"174,183"}},{"content":"<strong>IP Insights</strong>","children":[{"content":"<strong>Concept</strong>: Learns usage patterns for IPv4 addresses by associating them with entities (e.g., user IDs). Uses a neural network to detect anomalous logins from unusual IP addresses or locations and returns a high score for deviations.","children":[],"payload":{"tag":"li","lines":"184,185"}},{"content":"<strong>Example</strong>: If you always log in from home and then suddenly log in from another country, the algorithm flags it as anomalous and can trigger additional security checks.","children":[],"payload":{"tag":"li","lines":"185,186"}},{"content":"<strong>Problem Type</strong>: Anomaly Detection.","children":[],"payload":{"tag":"li","lines":"186,187"}},{"content":"<strong>Data Formats</strong>: <code>CSV</code> for training. <code>CSV</code>, <code>JSON</code>, <code>JSON Lines</code> for inference.","children":[],"payload":{"tag":"li","lines":"187,188"}},{"content":"<strong>Compute</strong>: Supports CPU &amp; GPU.","children":[],"payload":{"tag":"li","lines":"188,189"}},{"content":"<strong>Required Hyperparameters</strong>: <code>num_entity_vectors</code>, <code>vector_dim</code>.","children":[],"payload":{"tag":"li","lines":"189,190"}},{"content":"<strong>Metrics</strong>: Area Under Curve (AUC).","children":[],"payload":{"tag":"li","lines":"190,191"}},{"content":"<strong>Use Cases</strong>: Detecting fraudulent transactions/account takeovers, ensuring compliance with regional regulations, geolocation-based personalization.","children":[],"payload":{"tag":"li","lines":"191,192"}}],"payload":{"tag":"li","lines":"183,192"}},{"content":"<strong>K-Means</strong>","children":[{"content":"<strong>Concept</strong>: Groups data into a pre-determined number (<code>K</code>) of clusters. It iteratively assigns data points to the nearest cluster centroid (center point) and then recalculates the centroid based on the new members.","children":[],"payload":{"tag":"li","lines":"193,194"}},{"content":"<strong>Analogy</strong>: At a party, you want to create <code>K=3</code> interest groups. You pick 3 random &quot;leaders&quot; (centroids). Guests join the leader they have the most in common with. Then, new leaders are chosen based on the actual average interest of each group, and guests re-evaluate, repeating until the groups are stable.","children":[],"payload":{"tag":"li","lines":"194,195"}},{"content":"<strong>Problem Type</strong>: Clustering.","children":[],"payload":{"tag":"li","lines":"195,196"}},{"content":"<strong>Data Formats</strong>: <code>CSV</code>, <code>protobuf</code> for training. <code>JSON</code> for inference.","children":[],"payload":{"tag":"li","lines":"196,197"}},{"content":"<strong>Compute</strong>: Recommends CPU (GPU supported for single instance only).","children":[],"payload":{"tag":"li","lines":"197,198"}},{"content":"<strong>Required Hyperparameters</strong>: <code>feature_dim</code>, <code>K</code>.","children":[],"payload":{"tag":"li","lines":"198,199"}},{"content":"<strong>Metrics</strong>: Mean Square Distance (msd), Sum of Square Distance (ssd).","children":[],"payload":{"tag":"li","lines":"199,200"}},{"content":"<strong>Use Cases</strong>: Customer segmentation, market segmentation, recommendation systems.","children":[],"payload":{"tag":"li","lines":"200,202"}}],"payload":{"tag":"li","lines":"192,202"}}],"payload":{"tag":"h3","lines":"164,165"}},{"content":"5. Algorithms for Text Data (NLP)","children":[{"content":"<strong>Object2Vec</strong>","children":[{"content":"<strong>Concept</strong>: A customizable neural embedding algorithm that creates vector representations (embeddings) of various objects (e.g., sentences, users, products) by learning from their relationships. The goal is to adjust the vectors so that objects with similar relationships are closer together in the embedding space.","children":[],"payload":{"tag":"li","lines":"204,205"}},{"content":"<strong>Analogy</strong>: A librarian organizes books (objects) by creating a mental map (embedding space) where similar books are placed together. By observing what readers borrow, the librarian refines the map, making it easier to find related books.","children":[],"payload":{"tag":"li","lines":"205,206"}},{"content":"<strong>Problem Type</strong>: General Purpose Embedding.","children":[],"payload":{"tag":"li","lines":"206,207"}},{"content":"<strong>Data Formats</strong>: <code>JSON Lines</code> (sentence-sentence or label-sentence pairs) for training. <code>JSON</code> for inference.","children":[],"payload":{"tag":"li","lines":"207,208"}},{"content":"<strong>Compute</strong>: Supports CPU &amp; GPU.","children":[],"payload":{"tag":"li","lines":"208,209"}},{"content":"<strong>Required Hyperparameters</strong>: <code>enc0_max_seq_len</code>, <code>enc0_vocab_size</code>.","children":[],"payload":{"tag":"li","lines":"209,210"}},{"content":"<strong>Metrics</strong>: MSE (Regression); Accuracy, Cross-entropy (Classification).","children":[],"payload":{"tag":"li","lines":"210,211"}},{"content":"<strong>Use Cases</strong>: User behavior analysis, sentiment analysis, social network analysis.","children":[],"payload":{"tag":"li","lines":"211,212"}}],"payload":{"tag":"li","lines":"203,212"}},{"content":"<strong>Latent Dirichlet Allocation (LDA)</strong>","children":[{"content":"<strong>Concept</strong>: An unsupervised generative probabilistic model that discovers underlying topics in a collection of documents by analyzing word frequencies.","children":[],"payload":{"tag":"li","lines":"213,214"}},{"content":"<strong>Analogy</strong>: A librarian organizes a pile of books by guessing genres (topics) and then refining those genres by observing the common words within each book (e.g., &quot;detective,&quot; &quot;murder&quot; for the Mystery genre).","children":[],"payload":{"tag":"li","lines":"214,215"}},{"content":"<strong>Problem Type</strong>: Topic Modeling.","children":[],"payload":{"tag":"li","lines":"215,216"}},{"content":"<strong>Data Formats</strong>: <code>CSV</code>, <code>protobuf</code> for training. <code>JSON</code> for inference.","children":[],"payload":{"tag":"li","lines":"216,217"}},{"content":"<strong>Compute</strong>: Supports single-instance CPU only.","children":[],"payload":{"tag":"li","lines":"217,218"}},{"content":"<strong>Required Hyperparameters</strong>: <code>num_topics</code>, <code>feature_dim</code>, <code>mini_batch_size</code>.","children":[],"payload":{"tag":"li","lines":"218,219"}},{"content":"<strong>Metrics</strong>: Per-Word-Log-Likelihood (pwll).","children":[],"payload":{"tag":"li","lines":"219,220"}},{"content":"<strong>Use Cases</strong>: Analyzing customer feedback themes, identifying social media trends, generating content ideas.","children":[],"payload":{"tag":"li","lines":"220,221"}}],"payload":{"tag":"li","lines":"212,221"}},{"content":"<strong>Neural Topic Model (NTM)</strong>","children":[{"content":"<strong>Concept</strong>: An unsupervised algorithm, similar to LDA, but uses a neural network to model topics. It is more scalable than LDA but can be less interpretable due to the &quot;black box&quot; nature of neural networks.","children":[],"payload":{"tag":"li","lines":"222,223"}},{"content":"<strong>Problem Type</strong>: Topic Modeling.","children":[],"payload":{"tag":"li","lines":"223,224"}},{"content":"<strong>Data Formats</strong>: <code>CSV</code>, <code>protobuf</code> for training. <code>JSON</code>, <code>JSON Lines</code> for inference.","children":[],"payload":{"tag":"li","lines":"224,225"}},{"content":"<strong>Compute</strong>: Supports CPU &amp; GPU.","children":[],"payload":{"tag":"li","lines":"225,226"}},{"content":"<strong>Required Hyperparameters</strong>: <code>num_topics</code>, <code>feature_dim</code>.","children":[],"payload":{"tag":"li","lines":"226,227"}},{"content":"<strong>Metrics</strong>: Total loss.","children":[],"payload":{"tag":"li","lines":"227,228"}},{"content":"<strong>Use Cases</strong>: Uncovering customer pain points, personalized content recommendations, market sentiment analysis.","children":[],"payload":{"tag":"li","lines":"228,229"}}],"payload":{"tag":"li","lines":"221,229"}},{"content":"<strong>BlazingText</strong>","children":[{"content":"<strong>Concept</strong>: A highly optimized implementation of <code>Word2Vec</code> (for creating word embeddings) and <code>FastText</code> (for text classification). It is significantly faster than the original implementations, turning days of training into minutes.","children":[],"payload":{"tag":"li","lines":"230,231"}},{"content":"<strong>Modes</strong>:","children":[{"content":"<code>word2vec</code> (unsupervised): cbow, skip-gram, batch_skip-gram.","children":[],"payload":{"tag":"li","lines":"232,233"}},{"content":"<code>text classification</code> (supervised).","children":[],"payload":{"tag":"li","lines":"233,234"}}],"payload":{"tag":"li","lines":"231,234"}},{"content":"<strong>Data Formats</strong>: A single preprocessed text file (space-separated tokens). <code>JSON</code> for inference.","children":[],"payload":{"tag":"li","lines":"234,235"}},{"content":"<strong>Compute</strong>: Supports single CPU/GPU; multiple CPUs for batch_skip-gram.","children":[],"payload":{"tag":"li","lines":"235,236"}},{"content":"<strong>Required Hyperparameters</strong>: <code>mode</code>.","children":[],"payload":{"tag":"li","lines":"236,237"}},{"content":"<strong>Metrics</strong>: <code>mean_rho</code> (Word2Vec); Accuracy (Text Classification).","children":[],"payload":{"tag":"li","lines":"237,238"}},{"content":"<strong>Use Cases</strong>: Sentiment analysis, document classification, recommendation systems.","children":[],"payload":{"tag":"li","lines":"238,239"}}],"payload":{"tag":"li","lines":"229,239"}},{"content":"<strong>Sequence-to-Sequence (Seq2Seq)</strong>","children":[{"content":"<strong>Concept</strong>: A supervised algorithm that transforms an input sequence to an output sequence using an encoder-decoder neural network architecture. The encoder compresses the input into a feature vector, and the decoder converts that vector into the output sequence.","children":[],"payload":{"tag":"li","lines":"240,241"}},{"content":"<strong>Problem Type</strong>: Language Processing (Translation, Summarization).","children":[],"payload":{"tag":"li","lines":"241,242"}},{"content":"<strong>Data Formats</strong>: <code>protobuf</code> for training. <code>JSON</code>, <code>protobuf</code> for inference.","children":[],"payload":{"tag":"li","lines":"242,243"}},{"content":"<strong>Compute</strong>: Supports single-machine GPU only.","children":[],"payload":{"tag":"li","lines":"243,244"}},{"content":"<strong>Required Hyperparameters</strong>: None.","children":[],"payload":{"tag":"li","lines":"244,245"}},{"content":"<strong>Metrics</strong>: Accuracy, BLEU score, Perplexity.","children":[],"payload":{"tag":"li","lines":"245,246"}},{"content":"<strong>Use Cases</strong>: Machine translation, speech-to-text conversion, code generation.","children":[],"payload":{"tag":"li","lines":"246,248"}}],"payload":{"tag":"li","lines":"239,248"}}],"payload":{"tag":"h3","lines":"202,203"}},{"content":"6. Algorithms for Image Data","children":[{"content":"<strong>Image Classification</strong>","children":[{"content":"<strong>Concept</strong>: A supervised algorithm that classifies an entire image into one or more categories using a Convolutional Neural Network (CNN).","children":[],"payload":{"tag":"li","lines":"250,251"}},{"content":"<strong>Modes</strong>:","children":[{"content":"<code>Full training</code>: Train from scratch on a large dataset.","children":[],"payload":{"tag":"li","lines":"252,253"}},{"content":"<code>Transfer learning</code>: Fine-tune a pre-trained model on a smaller, specific dataset.","children":[],"payload":{"tag":"li","lines":"253,254"}}],"payload":{"tag":"li","lines":"251,254"}},{"content":"<strong>Data Formats</strong>: <code>recordIO</code> or image formats (<code>JPG</code>, <code>PNG</code>), plus a <code>.lst</code> file listing images.","children":[],"payload":{"tag":"li","lines":"254,255"}},{"content":"<strong>Compute</strong>: Recommends GPU for training.","children":[],"payload":{"tag":"li","lines":"255,256"}},{"content":"<strong>Required Hyperparameters</strong>: <code>num_classes</code>, <code>num_training_samples</code>.","children":[],"payload":{"tag":"li","lines":"256,257"}},{"content":"<strong>Metrics</strong>: Accuracy.","children":[],"payload":{"tag":"li","lines":"257,258"}},{"content":"<strong>Use Cases</strong>: Medical image diagnosis (X-rays), classifying objects for autonomous vehicles, security surveillance.","children":[],"payload":{"tag":"li","lines":"258,259"}}],"payload":{"tag":"li","lines":"249,259"}},{"content":"<strong>Object Detection</strong>","children":[{"content":"<strong>Concept</strong>: Goes beyond classification to identify and locate <em>multiple</em> objects within a single image by drawing bounding boxes around them. It uses the Single Shot MultiBox Detector (SSD) framework.","children":[],"payload":{"tag":"li","lines":"260,261"}},{"content":"<strong>Data Formats</strong>: <code>recordIO</code> or image formats (<code>JPG</code>, <code>PNG</code>), with a matching <code>.json</code> file for each image&apos;s annotations.","children":[],"payload":{"tag":"li","lines":"261,262"}},{"content":"<strong>Compute</strong>: Recommends GPU for training.","children":[],"payload":{"tag":"li","lines":"262,263"}},{"content":"<strong>Required Hyperparameters</strong>: <code>num_classes</code>, <code>num_training_samples</code>.","children":[],"payload":{"tag":"li","lines":"263,264"}},{"content":"<strong>Metrics</strong>: Mean Average Precision (mAP).","children":[],"payload":{"tag":"li","lines":"264,265"}},{"content":"<strong>Use Cases</strong>: Automated retail checkout systems, manufacturing quality control, scanning information from documents.","children":[],"payload":{"tag":"li","lines":"265,266"}}],"payload":{"tag":"li","lines":"259,266"}},{"content":"<strong>Semantic Segmentation</strong>","children":[{"content":"<strong>Concept</strong>: The most granular image task, assigning a class label to <em>every single pixel</em> in an image to understand object shapes. The output is a segmentation mask (a grayscale image where each shade represents a class).","children":[],"payload":{"tag":"li","lines":"267,268"}},{"content":"<strong>Data Formats</strong>: Requires separate directories for <code>train</code>/<code>validation</code> images (<code>JPG</code>) and their corresponding <code>train_annotation</code>/<code>validation_annotation</code> label maps (<code>PNG</code>).","children":[],"payload":{"tag":"li","lines":"268,269"}},{"content":"<strong>Compute</strong>: Recommends GPU for training.","children":[],"payload":{"tag":"li","lines":"269,270"}},{"content":"<strong>Required Hyperparameters</strong>: <code>num_classes</code>, <code>num_training_samples</code>.","children":[],"payload":{"tag":"li","lines":"270,271"}},{"content":"<strong>Metrics</strong>: Mean Intersection-over-Union (mIOU), Pixel Accuracy.","children":[],"payload":{"tag":"li","lines":"271,272"}},{"content":"<strong>Use Cases</strong>: Analyzing satellite imagery, retail shelf analysis, content moderation in media.","children":[],"payload":{"tag":"li","lines":"272,274"}}],"payload":{"tag":"li","lines":"266,274"}}],"payload":{"tag":"h3","lines":"248,249"}}],"payload":{"tag":"h2","lines":"82,83"}},{"content":"Task Statement 3.3: Model Training &amp; Optimization","children":[{"content":"1. Data Preparation for Training","children":[{"content":"<strong>Data Splitting</strong>: The process of splitting data to prevent overfitting and improve performance.","children":[{"content":"<strong>Training Data</strong>: Used for the model to learn hidden patterns.","children":[],"payload":{"tag":"li","lines":"280,281"}},{"content":"<strong>Validation Data</strong> (Optional): Used during training to measure performance and tune hyperparameters.","children":[],"payload":{"tag":"li","lines":"281,282"}},{"content":"<strong>Testing Data</strong>: Used after training to see how well the model generalizes to new, unseen data.","children":[],"payload":{"tag":"li","lines":"282,283"}}],"payload":{"tag":"li","lines":"279,283"}},{"content":"<strong>Cross-Validation</strong>: The process of validating a model against fresh data to get a stable estimate of its performance.","children":[{"content":"<strong>K-Fold Cross-Validation</strong>: The dataset is split into K folds. The model is trained on K-1 folds and tested on the remaining fold. This is repeated K times, and the average error is computed. It&apos;s effective but computationally expensive.","children":[],"payload":{"tag":"li","lines":"284,285"}},{"content":"<strong>Stratified K-Fold</strong>: Similar to K-Fold, but ensures each fold maintains the same class distribution as the entire dataset. Crucial for imbalanced classification problems (e.g., fraud detection).","children":[],"payload":{"tag":"li","lines":"285,286"}},{"content":"<strong>Leave-One-Out (LOOCV)</strong>: An extreme version where K equals the number of data points. Not often used due to high computational cost.","children":[],"payload":{"tag":"li","lines":"286,287"}}],"payload":{"tag":"li","lines":"283,287"}},{"content":"<strong>Data Shuffling</strong>: Randomizing the order of data to prevent the model from learning unintended patterns from the data&apos;s sequence, which enhances generalization. Many SageMaker built-in algorithms (XGBoost, Linear Learner) do this internally.","children":[],"payload":{"tag":"li","lines":"287,288"}},{"content":"<strong>Bootstrapping</strong>: A statistical technique of creating multiple data samples by resampling from the original dataset <em>with replacement</em>. It helps improve model stability and estimate confidence intervals for performance metrics.","children":[],"payload":{"tag":"li","lines":"288,290"}}],"payload":{"tag":"h3","lines":"278,279"}},{"content":"2. Optimization Techniques","children":[{"content":"<strong>Why Optimize?</strong>","children":[{"content":"To minimize the <strong>Loss Function</strong> (the difference between predicted and actual output).","children":[],"payload":{"tag":"li","lines":"292,293"}},{"content":"To ensure the model <strong>converges</strong> quickly, saving time and resources.","children":[],"payload":{"tag":"li","lines":"293,294"}},{"content":"To efficiently handle <strong>large datasets</strong> by updating parameters incrementally.","children":[],"payload":{"tag":"li","lines":"294,295"}},{"content":"To find the right <strong>bias-variance tradeoff</strong>, preventing overfitting and underfitting.","children":[],"payload":{"tag":"li","lines":"295,296"}}],"payload":{"tag":"li","lines":"291,296"}},{"content":"<strong>Gradient Descent</strong>: A common iterative algorithm used to find the minimum of a function (the loss function).","children":[{"content":"<strong>Learning Rate</strong>: The &quot;step size&quot; taken to reach the minimum error. Too large can overshoot the minimum; too small is computationally expensive.","children":[],"payload":{"tag":"li","lines":"297,298"}},{"content":"<strong>Convergence</strong>: The stable point reached at the end of the process.","children":[],"payload":{"tag":"li","lines":"298,299"}},{"content":"<strong>Local vs. Global Minimum</strong>: In non-convex problems, the algorithm can get stuck in a <em>local minimum</em>, which isn&apos;t the absolute lowest error point.","children":[],"payload":{"tag":"li","lines":"299,300"}}],"payload":{"tag":"li","lines":"296,300"}},{"content":"<strong>Stochastic Gradient Descent (SGD)</strong>: Updates model parameters using a randomly sampled subset (or single data point) at each iteration. The &quot;noise&quot; introduced helps escape local minima.","children":[],"payload":{"tag":"li","lines":"300,301"}},{"content":"<strong>Batch Gradient Descent</strong>: Updates model parameters only after calculating the gradient from the <em>entire</em> dataset. It&apos;s more stable but memory-intensive and infeasible for very large datasets.","children":[],"payload":{"tag":"li","lines":"301,303"}}],"payload":{"tag":"h3","lines":"290,291"}},{"content":"3. Choosing Compute Resources","children":[{"content":"<strong>CPUs vs. GPUs</strong>","children":[{"content":"<strong>Model Complexity</strong>: <strong>CPUs</strong> are fine for simple models (Linear/Logistic Regression); <strong>GPUs</strong> are better for complex deep learning models.","children":[],"payload":{"tag":"li","lines":"305,306"}},{"content":"<strong>Model Size</strong>: <strong>CPUs</strong> for smaller models; <strong>GPUs</strong> for larger models that require more memory.","children":[],"payload":{"tag":"li","lines":"306,307"}},{"content":"<strong>Inference Needs</strong>: <strong>CPUs</strong> for low-latency needs (e.g., real-time recommendations); <strong>GPUs</strong> for high-throughput batch processing.","children":[],"payload":{"tag":"li","lines":"307,308"}},{"content":"<strong>Cost</strong>: <strong>CPUs</strong> are cheaper; <strong>GPUs</strong> are more expensive but offer significant performance gains.","children":[],"payload":{"tag":"li","lines":"308,309"}}],"payload":{"tag":"li","lines":"304,309"}},{"content":"<strong>Recommended EC2 Instances</strong>","children":[{"content":"<strong>General Purpose</strong>: <code>m5.xlarge</code>, <code>m5.4xlarge</code>","children":[],"payload":{"tag":"li","lines":"310,311"}},{"content":"<strong>Compute Optimized</strong>: <code>c5.xlarge</code>, <code>c5.2xlarge</code>","children":[],"payload":{"tag":"li","lines":"311,312"}},{"content":"<strong>Accelerated Computing (GPU)</strong>: <code>p3.xlarge</code>, <code>p3.8xlarge</code>, <code>p4d.24xlarge</code>","children":[],"payload":{"tag":"li","lines":"312,313"}},{"content":"<strong>Tool</strong>: Use <strong>Amazon SageMaker Inference Recommender</strong> to find the ideal instance type and count for deployment.","children":[],"payload":{"tag":"li","lines":"313,314"}}],"payload":{"tag":"li","lines":"309,314"}},{"content":"<strong>Distributed Training</strong>: Training a model across multiple machines to reduce training time for large datasets or complex models.","children":[{"content":"<strong>Data Parallelism</strong>: Used for <strong>large datasets</strong>. The dataset is split across multiple GPUs, each with a full copy of the model. SageMaker provides the <strong>SMDDP</strong> library.","children":[],"payload":{"tag":"li","lines":"315,316"}},{"content":"<strong>Model Parallelism</strong>: Used for <strong>large models</strong>. The model itself is partitioned across multiple GPUs, and the dataset flows through them. SageMaker provides the <strong>SMP V2</strong> library.","children":[],"payload":{"tag":"li","lines":"316,317"}}],"payload":{"tag":"li","lines":"314,317"}},{"content":"<strong>Cost Optimization with Spot Training</strong>","children":[{"content":"<strong>Spot Instances</strong>: Unused EC2 capacity offered at up to a 90% discount. AWS can reclaim them with a 2-minute notice.","children":[],"payload":{"tag":"li","lines":"318,319"}},{"content":"<strong>Checkpoints</strong>: Crucial for spot training. SageMaker saves the model&apos;s progress to S3, so if the job is interrupted, it can restart from the last checkpoint.","children":[],"payload":{"tag":"li","lines":"319,320"}},{"content":"<strong>Configuration</strong>:","children":[{"content":"<code>use_spot_instances = True</code>","children":[],"payload":{"tag":"li","lines":"321,322"}},{"content":"<code>max_wait</code>: Max time to wait for a spot instance (must be &gt; <code>max_run</code>).","children":[],"payload":{"tag":"li","lines":"322,323"}},{"content":"<code>checkpoint_s3_uri</code>: The S3 location to save checkpoints.","children":[],"payload":{"tag":"li","lines":"323,325"}}],"payload":{"tag":"li","lines":"320,325"}}],"payload":{"tag":"li","lines":"317,325"}}],"payload":{"tag":"h3","lines":"303,304"}},{"content":"4. Debugging &amp; Monitoring","children":[{"content":"<strong>Amazon SageMaker Debugger</strong>: A tool to monitor, profile, and debug training jobs in real time.","children":[{"content":"<strong>Features</strong>:","children":[{"content":"<strong>Saves model state</strong> (weights, gradients, biases) at regular intervals.","children":[],"payload":{"tag":"li","lines":"328,329"}},{"content":"<strong>Detects common issues</strong> like non-converging loss, vanishing gradients, and resource bottlenecks.","children":[],"payload":{"tag":"li","lines":"329,330"}},{"content":"<strong>Stops training jobs early</strong> if problems are detected, saving time and money.","children":[],"payload":{"tag":"li","lines":"330,331"}}],"payload":{"tag":"li","lines":"327,331"}},{"content":"<strong>Built-in Rules</strong>: Use pre-built rules to identify problems automatically.","children":[],"payload":{"tag":"li","lines":"331,332"}},{"content":"<strong>Alerts</strong>: Integrate with <strong>Amazon CloudWatch</strong> and <strong>SNS</strong> to send notifications when anomalies occur.","children":[],"payload":{"tag":"li","lines":"332,333"}},{"content":"<strong>Best Practices</strong>: Use the client library for real-time analysis and visualization of system utilization.","children":[],"payload":{"tag":"li","lines":"333,335"}}],"payload":{"tag":"li","lines":"326,335"}}],"payload":{"tag":"h3","lines":"325,326"}},{"content":"5. Updating &amp; Retraining Models","children":[{"content":"<strong>Importance</strong>: Retraining models with new, fresh data is essential to maintain accuracy and relevance over time.","children":[],"payload":{"tag":"li","lines":"336,337"}},{"content":"<strong>Amazon SageMaker Canvas</strong>: A no-code, drag-and-drop UI for business users to create ML predictions.","children":[{"content":"<strong>Automated ML</strong>: Automatically selects the best algorithm and tunes hyperparameters based on the data.","children":[],"payload":{"tag":"li","lines":"338,339"}},{"content":"<strong>Retraining</strong>: Models can be retrained easily. Canvas can be configured to <strong>automatically update</strong> a model whenever its associated dataset is refreshed, ensuring predictions are always based on the latest data.","children":[],"payload":{"tag":"li","lines":"339,341"}}],"payload":{"tag":"li","lines":"337,341"}}],"payload":{"tag":"h3","lines":"335,336"}}],"payload":{"tag":"h2","lines":"276,277"}},{"content":"Task Statement 3.4: Hyperparameter Tuning &amp; Model Concepts","children":[{"content":"1. Key Concepts: Overfitting, Bias, and Variance","children":[{"content":"<strong>Overfitting</strong>: When a model performs well on training data but poorly on new, unseen data.","children":[{"content":"<strong>Analogy</strong>: A soccer team that perfectly beats their practice partner but fails against new opponents because they&apos;ve &quot;memorized&quot; one strategy instead of generalizing.","children":[],"payload":{"tag":"li","lines":"347,348"}},{"content":"<strong>Causes</strong>: The model is too complex and learns the noise in the training data as if it were a real signal.","children":[],"payload":{"tag":"li","lines":"348,349"}}],"payload":{"tag":"li","lines":"346,349"}},{"content":"<strong>Underfitting</strong>: When a model is too simple to capture the underlying patterns in the data, resulting in poor performance on both training and test data.","children":[],"payload":{"tag":"li","lines":"349,350"}},{"content":"<strong>The Bias-Variance Tradeoff</strong>:","children":[{"content":"<strong>Bias</strong>: Error from an overly simplistic model (underfitting). High bias models are consistent but inaccurate.","children":[{"content":"<strong>Bullseye Analogy</strong>: Shots are tightly clustered but far from the center.","children":[],"payload":{"tag":"li","lines":"352,353"}}],"payload":{"tag":"li","lines":"351,353"}},{"content":"<strong>Variance</strong>: Error from a model that&apos;s too sensitive to small fluctuations in the training data (overfitting). High variance models are accurate on training data but not on test data.","children":[{"content":"<strong>Bullseye Analogy</strong>: Shots are scattered all around the center.","children":[],"payload":{"tag":"li","lines":"354,355"}}],"payload":{"tag":"li","lines":"353,355"}},{"content":"<strong>Goal</strong>: Find a model with low bias and low variance.","children":[],"payload":{"tag":"li","lines":"355,357"}}],"payload":{"tag":"li","lines":"350,357"}}],"payload":{"tag":"h3","lines":"345,346"}},{"content":"2. Regularization (To Combat Overfitting)","children":[{"content":"<strong>Goal</strong>: Prevent overfitting by penalizing large model coefficients, especially with limited data or complex models.","children":[],"payload":{"tag":"li","lines":"358,359"}},{"content":"<strong>Techniques</strong>:","children":[{"content":"<strong>L1 Regularization (Lasso)</strong>: Adds a penalty proportional to the <em>absolute value</em> of coefficients. Can shrink irrelevant feature coefficients to zero, effectively performing feature selection.","children":[{"content":"<strong>Analogy</strong>: A coach telling the team to ignore factors they can&apos;t control (e.g., ticket sales) and focus only on the most important game parameters.","children":[],"payload":{"tag":"li","lines":"361,362"}}],"payload":{"tag":"li","lines":"360,362"}},{"content":"<strong>L2 Regularization (Ridge)</strong>: Adds a penalty proportional to the <em>square</em> of coefficients. It forces the model to distribute the impact of all important features more evenly rather than relying on just a few.","children":[{"content":"<strong>Analogy</strong>: A coach ensuring the team practices all aspects of the game (passing, dribbling, defense) equally, not just one or two.","children":[],"payload":{"tag":"li","lines":"363,364"}}],"payload":{"tag":"li","lines":"362,364"}},{"content":"<strong>Early Stopping</strong>: Stopping the training process at the &quot;sweet spot&quot; where the error on the validation data begins to increase, even as training error continues to decrease.","children":[],"payload":{"tag":"li","lines":"364,365"}},{"content":"<strong>Dropout</strong> (for Neural Networks): Randomly deactivates a fraction of neurons during each training iteration. This forces the remaining neurons to learn more robust and generalizable features.","children":[{"content":"<strong>Analogy</strong>: A coach randomly substituting players during practice so the team doesn&apos;t become overly reliant on any single player.","children":[],"payload":{"tag":"li","lines":"366,368"}}],"payload":{"tag":"li","lines":"365,368"}}],"payload":{"tag":"li","lines":"359,368"}}],"payload":{"tag":"h3","lines":"357,358"}},{"content":"3. Cross-Validation (To Assess Generalization)","children":[{"content":"<strong>Goal</strong>: Get a more reliable estimate of model performance on unseen data by training and testing on different subsets of the data.","children":[],"payload":{"tag":"li","lines":"369,370"}},{"content":"<strong>Techniques</strong>:","children":[{"content":"<strong>K-Fold Cross-Validation</strong>: The dataset is split into K folds. The model is trained K times, each time using a different fold as the test set and the rest for training.","children":[],"payload":{"tag":"li","lines":"371,372"}},{"content":"<strong>Stratified K-Fold</strong>: Essential for <strong>imbalanced datasets</strong>. It&apos;s similar to K-Fold but ensures that each fold maintains the same proportion of class labels as the original dataset.","children":[],"payload":{"tag":"li","lines":"372,373"}},{"content":"<strong>Time Series Split</strong>: Used for sequential data. Folds are created chronologically to ensure the model is always trained on past data and tested on future data, preserving temporal order.","children":[],"payload":{"tag":"li","lines":"373,375"}}],"payload":{"tag":"li","lines":"370,375"}}],"payload":{"tag":"h3","lines":"368,369"}},{"content":"4. Hyperparameter Tuning Strategies","children":[{"content":"<strong>Parameters vs. Hyperparameters</strong>","children":[{"content":"<strong>Parameters</strong>: Learned from the data during training (e.g., weights, biases).","children":[],"payload":{"tag":"li","lines":"377,378"}},{"content":"<strong>Hyperparameters</strong>: Set <em>before</em> training to define the model structure and training process (e.g., learning rate, number of layers).","children":[{"content":"<strong>Analogy</strong>: Hyperparameters are the &quot;design choices&quot; for a dress (fabric, style), while parameters are the specific &quot;measurements&quot; adjusted during tailoring.","children":[],"payload":{"tag":"li","lines":"379,380"}}],"payload":{"tag":"li","lines":"378,380"}}],"payload":{"tag":"li","lines":"376,380"}},{"content":"<strong>Tuning Approaches (AMT - Automatic Model Tuning in SageMaker)</strong>:","children":[{"content":"<strong>Grid Search</strong>: Exhaustively tries every possible combination of specified hyperparameter values. Best for a small number of categorical parameters.","children":[],"payload":{"tag":"li","lines":"381,382"}},{"content":"<strong>Random Search</strong>: Randomly samples a set number of combinations from the hyperparameter space. More efficient than Grid Search for high-dimensional spaces.","children":[],"payload":{"tag":"li","lines":"382,383"}},{"content":"<strong>Bayesian Optimization</strong>: Intelligently chooses the next hyperparameters to evaluate based on the results of previous trials. Most efficient but more complex.","children":[],"payload":{"tag":"li","lines":"383,385"}}],"payload":{"tag":"li","lines":"380,385"}}],"payload":{"tag":"h3","lines":"375,376"}},{"content":"5. Initializing Hyperparameter Ranges for Tuning","children":[{"content":"<strong>Parameter Ranges</strong>: Define the space for the tuning job to search.","children":[{"content":"<strong>Categorical</strong>: A list of discrete values to try.","children":[],"payload":{"tag":"li","lines":"387,388"}},{"content":"<strong>Continuous</strong>: A min/max range for a floating-point value.","children":[],"payload":{"tag":"li","lines":"388,389"}},{"content":"<strong>Integer</strong>: A min/max range for an integer value.","children":[],"payload":{"tag":"li","lines":"389,390"}}],"payload":{"tag":"li","lines":"386,390"}},{"content":"<strong>Scaling Options</strong>: How the tuning job searches the range.","children":[{"content":"<strong>Auto</strong>: SageMaker chooses the best scale.","children":[],"payload":{"tag":"li","lines":"391,392"}},{"content":"<strong>Linear</strong>: Searches the range evenly.","children":[],"payload":{"tag":"li","lines":"392,393"}},{"content":"<strong>Logarithmic</strong>: Best for ranges spanning several orders of magnitude.","children":[],"payload":{"tag":"li","lines":"393,394"}},{"content":"<strong>Reverse Logarithmic</strong>: For ranges between 0 and 1 that are sensitive to small changes.","children":[],"payload":{"tag":"li","lines":"394,396"}}],"payload":{"tag":"li","lines":"390,396"}}],"payload":{"tag":"h3","lines":"385,386"}},{"content":"6. Model-Specific Concepts &amp; Hyperparameters","children":[{"content":"<strong>Neural Network Architecture</strong>","children":[{"content":"<strong>Core Components</strong>:","children":[{"content":"<strong>Input Layer</strong>: Receives the data.","children":[],"payload":{"tag":"li","lines":"399,400"}},{"content":"<strong>Hidden Layer(s)</strong>: Perform transformations and derive insights.","children":[],"payload":{"tag":"li","lines":"400,401"}},{"content":"<strong>Output Layer</strong>: Makes the final prediction.","children":[],"payload":{"tag":"li","lines":"401,402"}}],"payload":{"tag":"li","lines":"398,402"}},{"content":"<strong>Key Concepts</strong>:","children":[{"content":"<strong>Weights &amp; Biases</strong>: Parameters adjusted during training to minimize error.","children":[],"payload":{"tag":"li","lines":"403,404"}},{"content":"<strong>Activation Functions</strong>: Introduce non-linearity (e.g., ReLU, Softmax, Sigmoid), allowing the network to learn complex patterns.","children":[],"payload":{"tag":"li","lines":"404,405"}}],"payload":{"tag":"li","lines":"402,405"}},{"content":"<strong>Key Hyperparameters</strong>:","children":[{"content":"<strong>Learning Rate</strong>: Step size for updating weights.","children":[],"payload":{"tag":"li","lines":"406,407"}},{"content":"<strong>Batch Size</strong>: Number of samples used in one iteration.","children":[],"payload":{"tag":"li","lines":"407,408"}},{"content":"<strong>Number of Epochs</strong>: Number of times the entire dataset is passed through the network.","children":[],"payload":{"tag":"li","lines":"408,409"}},{"content":"<strong>Number of Layers</strong>: The depth of the network.","children":[],"payload":{"tag":"li","lines":"409,410"}}],"payload":{"tag":"li","lines":"405,410"}}],"payload":{"tag":"li","lines":"397,410"}},{"content":"<strong>Tree-Based Models (e.g., Decision Trees)</strong>","children":[{"content":"<strong>Structure</strong>: A flow-chart like structure with a <strong>Root Node</strong>, intermediate <strong>Sub-nodes</strong> (decisions), <strong>Branches</strong> (outcomes), and <strong>Leaf Nodes</strong> (final predictions).","children":[],"payload":{"tag":"li","lines":"411,412"}},{"content":"<strong>Key Hyperparameters</strong>:","children":[{"content":"<code>max_depth</code>: The maximum depth of the tree.","children":[],"payload":{"tag":"li","lines":"413,414"}},{"content":"<code>min_samples_split</code>: The minimum number of samples required to split a node.","children":[],"payload":{"tag":"li","lines":"414,415"}}],"payload":{"tag":"li","lines":"412,415"}}],"payload":{"tag":"li","lines":"410,415"}},{"content":"<strong>Linear Models</strong>","children":[{"content":"<strong>Key Hyperparameters</strong>:","children":[{"content":"<strong>Learning Rate</strong>: Controls the speed of convergence in gradient descent.","children":[],"payload":{"tag":"li","lines":"417,418"}},{"content":"<strong>Alpha</strong>: The regularization strength parameter.","children":[],"payload":{"tag":"li","lines":"418,420"}}],"payload":{"tag":"li","lines":"416,420"}}],"payload":{"tag":"li","lines":"415,420"}}],"payload":{"tag":"h3","lines":"396,397"}},{"content":"7. SageMaker Automatic Model Tuning (AMT) in Practice","children":[{"content":"<strong>Best Practices</strong>:","children":[{"content":"Limit the number of hyperparameters to search.","children":[],"payload":{"tag":"li","lines":"422,423"}},{"content":"Choose reasonable, smaller ranges.","children":[],"payload":{"tag":"li","lines":"423,424"}},{"content":"Limit the number of concurrent training jobs to allow the tuning to learn from previous runs.","children":[],"payload":{"tag":"li","lines":"424,425"}}],"payload":{"tag":"li","lines":"421,425"}},{"content":"<strong>Early Stopping</strong>: Automatically terminates training jobs that are not performing well compared to the best job so far, saving time and money. Set <code>early_stopping_type</code> to <code>auto</code>.","children":[],"payload":{"tag":"li","lines":"425,426"}},{"content":"<strong>Warm Start</strong>: Reuses knowledge from a previous tuning job to inform and speed up a new one. The new job doesn&apos;t start from scratch.","children":[],"payload":{"tag":"li","lines":"426,427"}},{"content":"<strong>Resource Limits</strong>: Be aware of account limits (e.g., max concurrent jobs, max total jobs, max hyperparameters per job).","children":[],"payload":{"tag":"li","lines":"427,429"}}],"payload":{"tag":"h3","lines":"420,421"}}],"payload":{"tag":"h2","lines":"343,344"}},{"content":"Task Statement 3.5: Model Evaluation","children":[{"content":"1. Evaluate Metrics","children":[{"content":"<strong>Classification</strong>: Accuracy, Precision, Recall, F1 Score, AUC Curve.","children":[],"payload":{"tag":"li","lines":"434,435"}},{"content":"<strong>Regression</strong>: MAE, MSE, RMSE.","children":[],"payload":{"tag":"li","lines":"435,437"}}],"payload":{"tag":"h3","lines":"433,434"}},{"content":"2. Interpret Confusion Matrices","children":[{"content":"<strong>Purpose</strong>: A table showing the performance of a classification model.","children":[],"payload":{"tag":"li","lines":"438,439"}},{"content":"<strong>Components</strong>: True Positive (TP), True Negative (TN), False Positive (FP), False Negative (FN).","children":[],"payload":{"tag":"li","lines":"439,441"}}],"payload":{"tag":"h3","lines":"437,438"}},{"content":"3. Online and Offline Model Evaluation","children":[{"content":"<strong>Online Evaluation (A/B Testing)</strong>: Assessing model performance on live data in production.","children":[],"payload":{"tag":"li","lines":"442,444"}}],"payload":{"tag":"h3","lines":"441,442"}},{"content":"4. Compare ML Models","children":[{"content":"<strong>Consider</strong>: Computational Complexity (Time, Space), Sample Complexity.","children":[],"payload":{"tag":"li","lines":"445,446"}}],"payload":{"tag":"h3","lines":"444,445"}}],"payload":{"tag":"h2","lines":"431,432"}}],"payload":{"tag":"h1","lines":"8,9"}},{"colorFreezeLevel":2,"initialExpandLevel":2,"maxWidth":350})</script>
</body>
</html>
